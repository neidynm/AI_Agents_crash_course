{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b50e3e2-fef7-48d5-ab02-d34081fe15ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "import logging\n",
    "import asyncio\n",
    "from tqdm import tqdm\n",
    "from minsearch import Index\n",
    "from typing import List, Any, Dict, Tuple, Optional\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pydantic_ai import Agent\n",
    "import json\n",
    "import secrets\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pydantic_ai.messages import ModelMessagesTypeAdapter\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e4900d3-1f53-4bde-910c-591345b732b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_repo_data(repo_owner, repo_name, branch=\"main\"):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    Yields one document (dict) at a time to avoid loading everything into memory.\n",
    "\n",
    "    Args:\n",
    "        repo_owner (str): GitHub username or organization\n",
    "        repo_name (str): Repository name\n",
    "        branch (str): Branch name (default: main)\n",
    "    \"\"\"\n",
    "    url = f\"https://codeload.github.com/{repo_owner}/{repo_name}/zip/refs/heads/{branch}\"\n",
    "    resp = requests.get(url)\n",
    "\n",
    "    if resp.status_code == 404 and branch == \"main\":\n",
    "        # Try fallback to master\n",
    "        return read_repo_data(repo_owner, repo_name, branch=\"master\")\n",
    "\n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: HTTP {resp.status_code}\")\n",
    "\n",
    "    with zipfile.ZipFile(io.BytesIO(resp.content)) as zf:\n",
    "        for file_info in zf.infolist():\n",
    "            filename = file_info.filename\n",
    "            if not filename.lower().endswith((\".md\", \".mdx\")):\n",
    "                continue\n",
    "            try:\n",
    "                with zf.open(file_info) as f_in:\n",
    "                    content = f_in.read().decode(\"utf-8\", errors=\"replace\")\n",
    "                    post = frontmatter.loads(content)\n",
    "                    data = post.to_dict()\n",
    "                    data.update({\n",
    "                        \"filename\": filename,\n",
    "                        \"repo\": repo_name,\n",
    "                        \"owner\": repo_owner,\n",
    "                        \"branch\": branch\n",
    "                    })\n",
    "                    yield data\n",
    "            except Exception as e:\n",
    "                logging.warning(\"Error processing %s: %s\", filename, e)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a7494ff-4c51-4680-8b4b-0751139ee23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq, size, step):\n",
    "    \"\"\"Yield overlapping chunks from a long string.\"\"\"\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "    n = len(seq)\n",
    "    for i in range(0, n, step):\n",
    "        yield {\"start\": i, \"chunk\": seq[i:i+size]}\n",
    "        if i + size >= n:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a79dc7e-7341-4ca0-b103-0f9916fb5283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 95it [00:01, 58.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 575 chunks. Building index...\n",
      "Text indexing complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evidently_chunks = []\n",
    "\n",
    "for doc in tqdm(read_repo_data(\"evidentlyai\", \"docs\"), desc=\"Processing files\"):\n",
    "    doc_copy = doc.copy()\n",
    "    content = doc_copy.pop(\"content\", \"\")\n",
    "    for chunk in sliding_window(content, size=2000, step=1000):\n",
    "        chunk.update(doc_copy)\n",
    "        evidently_chunks.append(chunk)\n",
    "\n",
    "print(f\"Collected {len(evidently_chunks)} chunks. Building index...\")\n",
    "\n",
    "# Build text search index\n",
    "index = Index(\n",
    "    text_fields=[\"chunk\", \"title\", \"description\", \"filename\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "index.fit(evidently_chunks)\n",
    "\n",
    "print(\"Text indexing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46836ed8-7782-4545-a40d-c86a51222df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorSearch:\n",
    "    \"\"\"\n",
    "    Simple vector search implementation using cosine similarity.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.embeddings = None\n",
    "        self.documents = None\n",
    "    \n",
    "    def fit(self, embeddings: np.ndarray, documents: List[Dict]):\n",
    "        \"\"\"\n",
    "        Store embeddings and associated documents.\n",
    "        \"\"\"\n",
    "        self.embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)  # Normalize\n",
    "        self.documents = documents\n",
    "    \n",
    "    def search(self, query_embedding: np.ndarray, num_results: int = 5) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search for most similar documents using cosine similarity.\n",
    "        \"\"\"\n",
    "        if self.embeddings is None:\n",
    "            return []\n",
    "        \n",
    "        # Normalize query embedding\n",
    "        query_norm = query_embedding / np.linalg.norm(query_embedding)\n",
    "        \n",
    "        # Calculate cosine similarities\n",
    "        similarities = np.dot(self.embeddings, query_norm)\n",
    "        \n",
    "        # Get top k indices\n",
    "        top_indices = np.argsort(similarities)[-num_results:][::-1]\n",
    "        \n",
    "        # Return documents with similarity scores\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            doc = self.documents[idx].copy()\n",
    "            doc['similarity_score'] = float(similarities[idx])\n",
    "            results.append(doc)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "178d1699-ec0c-4073-987c-a47fd1a68bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings for semantic search...\n",
      "Loading embedding model...\n",
      "Encoding document chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 575/575 [05:21<00:00,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector indexing complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating embeddings for semantic search...\")\n",
    "\n",
    "# Initialize an empty list to store embeddings for each chunk\n",
    "evidently_embeddings = []\n",
    "\n",
    "# Load a pre-trained sentence transformer model for creating embeddings\n",
    "print(\"Loading embedding model...\")\n",
    "embedding_model = SentenceTransformer('multi-qa-distilbert-cos-v1')\n",
    "\n",
    "# Loop through each document chunk in evidently_chunks\n",
    "print(\"Encoding document chunks...\")\n",
    "for d in tqdm(evidently_chunks, desc=\"Creating embeddings\"):\n",
    "    # Create a combined text for better context\n",
    "    text_to_encode = d['chunk']\n",
    "    if 'title' in d and d['title']:\n",
    "        text_to_encode = f\"{d['title']}. {text_to_encode}\"\n",
    "    if 'filename' in d:\n",
    "        # Extract meaningful parts from filename\n",
    "        filename_parts = d['filename'].replace('/', ' ').replace('_', ' ').replace('.mdx', '').replace('.md', '')\n",
    "        text_to_encode = f\"{filename_parts}. {text_to_encode}\"\n",
    "    \n",
    "    # Encode the enhanced text into a vector (embedding)\n",
    "    v = embedding_model.encode(text_to_encode, show_progress_bar=False)\n",
    "    \n",
    "    # Append the embedding to the list\n",
    "    evidently_embeddings.append(v)\n",
    "\n",
    "# Convert the list of embeddings into a NumPy array\n",
    "evidently_embeddings = np.array(evidently_embeddings)\n",
    "\n",
    "# Initialize and fit vector search index\n",
    "evidently_vindex = VectorSearch()\n",
    "evidently_vindex.fit(evidently_embeddings, evidently_chunks)\n",
    "\n",
    "print(\"Vector indexing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a45f43ee-427f-4632-8aa5-8d7ee7f7fc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenRouter Connection Test: \n"
     ]
    }
   ],
   "source": [
    "openai_client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\")\n",
    ")\n",
    "\n",
    "# Test the connection\n",
    "try:\n",
    "    test_response = openai_client.chat.completions.create(\n",
    "        model=\"deepseek/deepseek-r1:free\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Say 'Connection successful' if you can read this.\"}],\n",
    "        max_tokens=50\n",
    "    )\n",
    "    print(f\"OpenRouter Connection Test: {test_response.choices[0].message.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Connection Error: {e}\")\n",
    "    print(\"Make sure you have OPENROUTER_API_KEY in your .env file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2c87751-6e85-4cec-aee6-49fc0bc9f46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Testing search methods:\n",
      "Text search found 5 results for query: 'test dataset'\n",
      "Vector search found 5 results for query: 'test dataset'\n",
      "Hybrid search found 5 results for query: 'test dataset'\n"
     ]
    }
   ],
   "source": [
    "def text_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Perform a text-based search on the FAQ index.\n",
    "    \"\"\"\n",
    "    results = index.search(query, num_results=5)\n",
    "    print(f\"Text search found {len(results)} results for query: '{query}'\")\n",
    "    return results\n",
    "\n",
    "def vector_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Perform semantic vector-based search.\n",
    "    \"\"\"\n",
    "    # Encode the query into a vector using the embedding model\n",
    "    q = embedding_model.encode(query)\n",
    "    # Search the vector index for the top 5 most similar chunks\n",
    "    results = evidently_vindex.search(q, num_results=5)\n",
    "    print(f\"Vector search found {len(results)} results for query: '{query}'\")\n",
    "    return results\n",
    "\n",
    "def hybrid_search(query: str, alpha: float = 0.5, num_results: int = 5) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Perform hybrid search combining text and vector search.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        alpha: Weight for text search (1-alpha for vector search)\n",
    "        num_results: Number of results to return\n",
    "    \"\"\"\n",
    "    # Get results from both search methods\n",
    "    text_results = index.search(query, num_results=num_results*2)\n",
    "    \n",
    "    q_embedding = embedding_model.encode(query)\n",
    "    vector_results = evidently_vindex.search(q_embedding, num_results=num_results*2)\n",
    "    \n",
    "    # Create a scoring dictionary\n",
    "    doc_scores = {}\n",
    "    \n",
    "    # Add text search scores\n",
    "    for i, doc in enumerate(text_results):\n",
    "        doc_key = doc.get('filename', '') + str(doc.get('start', 0))\n",
    "        # Use inverse rank as score\n",
    "        text_score = 1.0 / (i + 1)\n",
    "        doc_scores[doc_key] = {\n",
    "            'doc': doc,\n",
    "            'text_score': text_score * alpha,\n",
    "            'vector_score': 0\n",
    "        }\n",
    "    \n",
    "    # Add vector search scores\n",
    "    for doc in vector_results:\n",
    "        doc_key = doc.get('filename', '') + str(doc.get('start', 0))\n",
    "        vector_score = doc.get('similarity_score', 0) * (1 - alpha)\n",
    "        \n",
    "        if doc_key in doc_scores:\n",
    "            doc_scores[doc_key]['vector_score'] = vector_score\n",
    "        else:\n",
    "            doc_scores[doc_key] = {\n",
    "                'doc': doc,\n",
    "                'text_score': 0,\n",
    "                'vector_score': vector_score\n",
    "            }\n",
    "    \n",
    "    # Calculate combined scores\n",
    "    for key in doc_scores:\n",
    "        doc_scores[key]['combined_score'] = doc_scores[key]['text_score'] + doc_scores[key]['vector_score']\n",
    "    \n",
    "    # Sort by combined score and return top results\n",
    "    sorted_docs = sorted(doc_scores.values(), key=lambda x: x['combined_score'], reverse=True)\n",
    "    results = [item['doc'] for item in sorted_docs[:num_results]]\n",
    "    \n",
    "    print(f\"Hybrid search found {len(results)} results for query: '{query}'\")\n",
    "    return results\n",
    "\n",
    "# Test all search methods\n",
    "test_query = \"test dataset\"\n",
    "print(\"\\n Testing search methods:\")\n",
    "text_results = text_search(test_query)\n",
    "vector_results = vector_search(test_query)\n",
    "hybrid_results = hybrid_search(test_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6107d19-bc52-41d6-95aa-8a82f073622a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â“ Question: What components are required in a test dataset to evaluate AI?\n",
      "Thinking (using hybrid search + LLM approach)...\n",
      "Hybrid search found 5 results for query: 'What components are required in a test dataset to evaluate AI?'\n",
      "\n",
      " Answer:\n",
      "To evaluate an AI system (specifically a RAG system) using Evidently, the test dataset should include:\n",
      "\n",
      "1. **User-like questions** - Automatically generated queries that mimic real user inputs\n",
      "2. **Ground truth answers** - Corresponding correct answers derived directly from the knowledge base\n",
      "3. **Optional context** - Source material from your knowledge base that was used to generate each answer (can be included if needed)\n",
      "\n",
      "These components are generated automatically when using Evidently's RAG test dataset creation tool. The dataset can be refined by adding variations, removing irrelevant cases, or manually editing questions/responses before use in evaluation.\n"
     ]
    }
   ],
   "source": [
    "def answer_question_manual(question: str, search_method: str = 'hybrid') -> str:\n",
    "    \"\"\"\n",
    "    Answer a question by searching and then using the LLM.\n",
    "    \n",
    "    Args:\n",
    "        question: The question to answer\n",
    "        search_method: 'text', 'vector', or 'hybrid'\n",
    "    \"\"\"\n",
    "    # Select search method\n",
    "    if search_method == 'text':\n",
    "        search_results = text_search(question)\n",
    "    elif search_method == 'vector':\n",
    "        search_results = vector_search(question)\n",
    "    else:  # hybrid\n",
    "        search_results = hybrid_search(question)\n",
    "\n",
    "    # Format the search results as context\n",
    "    context = \"\\n\\n---\\n\\n\".join([\n",
    "        f\"Result {i+1} (from {result.get('filename', 'unknown')}):\\n{result.get('chunk', '')}\"\n",
    "        for i, result in enumerate(search_results)\n",
    "    ])\n",
    "\n",
    "    # Create the prompt with the context\n",
    "    prompt = f\"\"\"You are an expert assistant that answers questions about the Evidently project \n",
    "(https://github.com/evidentlyai/evidently) using ONLY the information provided in the context below.\n",
    "\n",
    "Context from Evidently documentation:\n",
    "{context}\n",
    "\n",
    "User question: {question}\n",
    "\n",
    "Instructions:\n",
    "- Answer based ONLY on the provided context\n",
    "- Be concise and clear\n",
    "- If the answer is not in the context, say \"I could not find this information in the Evidently documentation\"\n",
    "- Do not invent features or functionality\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"deepseek/deepseek-r1:free\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions about Evidently.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=1000\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error getting answer: {e}\"\n",
    "\n",
    "# Test question answering\n",
    "question = \"What components are required in a test dataset to evaluate AI?\"\n",
    "print(f\"\\nâ“ Question: {question}\")\n",
    "print(\"Thinking (using hybrid search + LLM approach)...\")\n",
    "answer = answer_question_manual(question, search_method='hybrid')\n",
    "print(f\"\\n Answer:\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a50838ff-e46c-4b8a-ad4e-d0b226c31fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = Path(os.getenv('LOGS_DIRECTORY', 'logs'))\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def log_entry(agent, messages, source=\"user\"):\n",
    "    tools = []\n",
    "    for ts in agent.toolsets:\n",
    "        tools.extend(ts.tools.keys())\n",
    "    dict_messages = ModelMessagesTypeAdapter.dump_python(messages)\n",
    "    return {\n",
    "        \"agent_name\": agent.name,\n",
    "        \"system_prompt\": agent._instructions,\n",
    "        \"provider\": agent.model.system,\n",
    "        \"model\": agent.model.model_name,\n",
    "        \"tools\": tools,\n",
    "        \"messages\": dict_messages,\n",
    "        \"source\": source\n",
    "    }\n",
    "\n",
    "def serializer(obj):\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "def log_interaction_to_file(agent, messages, source='user'):\n",
    "    entry = log_entry(agent, messages, source)\n",
    "    ts = entry['messages'][-1]['timestamp']\n",
    "    ts_str = ts.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    rand_hex = secrets.token_hex(3)\n",
    "    filename = f\"{agent.name}_{ts_str}_{rand_hex}.json\"\n",
    "    filepath = LOG_DIR / filename\n",
    "    with filepath.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(entry, f_out, indent=2, default=serializer)\n",
    "    return filepath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b9a816e-c018-4d5b-95dc-a40596c03b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_search_quality(\n",
    "    search_function, \n",
    "    test_queries: List[Tuple[str, List[str]]], \n",
    "    num_results: int = 5,\n",
    "    log_results: bool = True\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate search quality using hit rate and MRR metrics.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    timestamp = datetime.now()\n",
    "\n",
    "    print(f\"Starting evaluation with {len(test_queries)} test queries...\")\n",
    "\n",
    "    for idx, (query, expected_docs) in enumerate(test_queries, 1):\n",
    "        print(f\"  Query {idx}/{len(test_queries)}: '{query[:50]}...'\")\n",
    "\n",
    "        try:\n",
    "            # Execute search\n",
    "            if hasattr(search_function, '__self__'):  # Method\n",
    "                search_results = search_function(query, num_results=num_results)\n",
    "            else:  # Function\n",
    "                search_results = search_function(query)[:num_results]\n",
    "\n",
    "            # Extract filenames from results\n",
    "            retrieved_docs = [doc.get('filename', '') for doc in search_results]\n",
    "\n",
    "            # Calculate hit rate (binary: found at least one relevant doc)\n",
    "            relevant_found = any(doc in expected_docs for doc in retrieved_docs)\n",
    "\n",
    "            # Calculate MRR (Mean Reciprocal Rank)\n",
    "            mrr = 0\n",
    "            first_relevant_rank = None\n",
    "            for i, doc in enumerate(retrieved_docs):\n",
    "                if doc in expected_docs:\n",
    "                    mrr = 1 / (i + 1)\n",
    "                    first_relevant_rank = i + 1\n",
    "                    break\n",
    "\n",
    "            # Calculate Precision@k\n",
    "            relevant_in_results = sum(1 for doc in retrieved_docs if doc in expected_docs)\n",
    "            precision_at_k = relevant_in_results / len(retrieved_docs) if retrieved_docs else 0\n",
    "\n",
    "            # Store detailed result\n",
    "            result = {\n",
    "                'query': query,\n",
    "                'expected_docs': expected_docs,\n",
    "                'retrieved_docs': retrieved_docs,\n",
    "                'hit': relevant_found,\n",
    "                'mrr': mrr,\n",
    "                'precision_at_k': precision_at_k,\n",
    "                'first_relevant_rank': first_relevant_rank,\n",
    "                'num_relevant_found': relevant_in_results\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {e}\")\n",
    "            results.append({\n",
    "                'query': query,\n",
    "                'expected_docs': expected_docs,\n",
    "                'retrieved_docs': [],\n",
    "                'hit': False,\n",
    "                'mrr': 0,\n",
    "                'precision_at_k': 0,\n",
    "                'first_relevant_rank': None,\n",
    "                'num_relevant_found': 0,\n",
    "                'error': str(e)\n",
    "            })\n",
    "\n",
    "    # Calculate aggregate metrics\n",
    "    hit_rate = sum(r['hit'] for r in results) / len(results) if results else 0\n",
    "    avg_mrr = sum(r['mrr'] for r in results) / len(results) if results else 0\n",
    "    avg_precision = sum(r['precision_at_k'] for r in results) / len(results) if results else 0\n",
    "\n",
    "    evaluation_summary = {\n",
    "        'timestamp': timestamp.isoformat(),\n",
    "        'num_queries': len(test_queries),\n",
    "        'num_results_per_query': num_results,\n",
    "        'metrics': {\n",
    "            'hit_rate': hit_rate,\n",
    "            'mean_reciprocal_rank': avg_mrr,\n",
    "            'mean_precision_at_k': avg_precision\n",
    "        },\n",
    "        'detailed_results': results\n",
    "    }\n",
    "\n",
    "    # Log results if requested\n",
    "    if log_results:\n",
    "        log_evaluation_results(evaluation_summary)\n",
    "\n",
    "    return evaluation_summary\n",
    "\n",
    "def log_evaluation_results(evaluation_data: Dict[str, Any]) -> Path:\n",
    "    \"\"\"\n",
    "    Log evaluation results to a JSON file.\n",
    "    \"\"\"\n",
    "    ts = datetime.fromisoformat(evaluation_data['timestamp'])\n",
    "    ts_str = ts.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    rand_hex = secrets.token_hex(3)\n",
    "    filename = f\"search_evaluation_{ts_str}_{rand_hex}.json\"\n",
    "    filepath = LOG_DIR / filename\n",
    "    with filepath.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(evaluation_data, f_out, indent=2, default=str)\n",
    "    print(f\"ðŸ“ Evaluation results saved to: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "def print_evaluation_report(evaluation_summary: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Print a formatted evaluation report.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ“Š EVALUATION REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Timestamp: {evaluation_summary['timestamp']}\")\n",
    "    print(f\"Number of queries: {evaluation_summary['num_queries']}\")\n",
    "    print(f\"Results per query: {evaluation_summary['num_results_per_query']}\")\n",
    "\n",
    "    print(\"\\nðŸ“ˆ AGGREGATE METRICS:\")\n",
    "    metrics = evaluation_summary['metrics']\n",
    "    print(f\"  â€¢ Hit Rate: {metrics['hit_rate']:.2%}\")\n",
    "    print(f\"  â€¢ Mean Reciprocal Rank (MRR): {metrics['mean_reciprocal_rank']:.3f}\")\n",
    "    print(f\"  â€¢ Mean Precision@{evaluation_summary['num_results_per_query']}: {metrics['mean_precision_at_k']:.2%}\")\n",
    "\n",
    "    print(\"\\nðŸ” QUERY-LEVEL RESULTS:\")\n",
    "    for i, result in enumerate(evaluation_summary['detailed_results'][:5], 1):\n",
    "        print(f\"\\n  Query {i}: \\\"{result['query'][:50]}...\\\"\")\n",
    "        print(f\"    â€¢ Hit: {'âœ…' if result['hit'] else 'âŒ'}\")\n",
    "        print(f\"    â€¢ MRR: {result['mrr']:.3f}\")\n",
    "        print(f\"    â€¢ Precision: {result['precision_at_k']:.2%}\")\n",
    "        if result['first_relevant_rank']:\n",
    "            print(f\"    â€¢ First relevant at rank: {result['first_relevant_rank']}\")\n",
    "\n",
    "    if len(evaluation_summary['detailed_results']) > 5:\n",
    "        print(f\"\\n  ... and {len(evaluation_summary['detailed_results']) - 5} more queries\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "891dda51-3343-4172-81c7-e6702f2cad6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = [\n",
    "    # Testing and evaluation\n",
    "    (\"What components are required in a test dataset to evaluate AI?\", \n",
    "     [\"docs-main/docs/library/tests.mdx\", \n",
    "      \"docs-main/examples/LLM_regression_testing.mdx\"]),\n",
    "\n",
    "    (\"How to run evaluations in Evidently?\", \n",
    "     [\"docs-main/docs/library/evaluations_overview.mdx\",\n",
    "      \"docs-main/docs/library/tests.mdx\"]),\n",
    "\n",
    "    # Data drift\n",
    "    (\"Data drift detection methods\",\n",
    "     [\"docs-main/metrics/preset_data_drift.mdx\",\n",
    "      \"docs-main/metrics/customize_data_drift.mdx\"]),\n",
    "\n",
    "    (\"How to customize embedding drift detection?\",\n",
    "     [\"docs-main/metrics/customize_embedding_drift.mdx\",\n",
    "      \"docs-main/metrics/explainer_drift.mdx\"]),\n",
    "\n",
    "    # Monitoring and dashboards\n",
    "    (\"How to create monitoring dashboards?\",\n",
    "     [\"docs-main/docs/platform/dashboard_overview.mdx\",\n",
    "      \"docs-main/docs/platform/dashboard_add_panels.mdx\"]),\n",
    "\n",
    "    (\"Dashboard panel types and configuration\",\n",
    "     [\"docs-main/docs/platform/dashboard_panel_types.mdx\",\n",
    "      \"docs-main/docs/platform/dashboard_add_panels_ui.mdx\"]),\n",
    "\n",
    "    # Reports and output formats\n",
    "    (\"How to generate reports in Evidently?\",\n",
    "     [\"docs-main/docs/library/report.mdx\",\n",
    "      \"docs-main/docs/library/output_formats.mdx\"]),\n",
    "\n",
    "    # Synthetic data\n",
    "    (\"Generate synthetic test data\",\n",
    "     [\"docs-main/synthetic-data/introduction.mdx\",\n",
    "      \"docs-main/synthetic-data/input_data.mdx\",\n",
    "      \"docs-main/docs/library/synthetic_data_api.mdx\"]),\n",
    "\n",
    "    # Alerts and monitoring\n",
    "    (\"Setting up alerts for model monitoring\",\n",
    "     [\"docs-main/docs/platform/alerts.mdx\",\n",
    "      \"docs-main/docs/platform/dashboard_overview.mdx\"]),\n",
    "\n",
    "    # Data and metrics\n",
    "    (\"Understanding data definition and descriptors\",\n",
    "     [\"docs-main/docs/library/data_definition.mdx\",\n",
    "      \"docs-main/docs/library/descriptors.mdx\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9e5db7f-5215-47f3-ac35-f5bc283f9342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " COMPARING SEARCH METHODS\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š Evaluating TEXT search...\n",
      "Starting evaluation with 10 test queries...\n",
      "  Query 1/10: 'What components are required in a test dataset to ...'\n",
      "Text search found 5 results for query: 'What components are required in a test dataset to evaluate AI?'\n",
      "  Query 2/10: 'How to run evaluations in Evidently?...'\n",
      "Text search found 5 results for query: 'How to run evaluations in Evidently?'\n",
      "  Query 3/10: 'Data drift detection methods...'\n",
      "Text search found 5 results for query: 'Data drift detection methods'\n",
      "  Query 4/10: 'How to customize embedding drift detection?...'\n",
      "Text search found 5 results for query: 'How to customize embedding drift detection?'\n",
      "  Query 5/10: 'How to create monitoring dashboards?...'\n",
      "Text search found 5 results for query: 'How to create monitoring dashboards?'\n",
      "  Query 6/10: 'Dashboard panel types and configuration...'\n",
      "Text search found 5 results for query: 'Dashboard panel types and configuration'\n",
      "  Query 7/10: 'How to generate reports in Evidently?...'\n",
      "Text search found 5 results for query: 'How to generate reports in Evidently?'\n",
      "  Query 8/10: 'Generate synthetic test data...'\n",
      "Text search found 5 results for query: 'Generate synthetic test data'\n",
      "  Query 9/10: 'Setting up alerts for model monitoring...'\n",
      "Text search found 5 results for query: 'Setting up alerts for model monitoring'\n",
      "  Query 10/10: 'Understanding data definition and descriptors...'\n",
      "Text search found 5 results for query: 'Understanding data definition and descriptors'\n",
      "  â€¢ Hit Rate: 60.00%\n",
      "  â€¢ MRR: 0.550\n",
      "  â€¢ Precision@5: 40.00%\n",
      "\n",
      "ðŸ“Š Evaluating VECTOR search...\n",
      "Starting evaluation with 10 test queries...\n",
      "  Query 1/10: 'What components are required in a test dataset to ...'\n",
      "Vector search found 5 results for query: 'What components are required in a test dataset to evaluate AI?'\n",
      "  Query 2/10: 'How to run evaluations in Evidently?...'\n",
      "Vector search found 5 results for query: 'How to run evaluations in Evidently?'\n",
      "  Query 3/10: 'Data drift detection methods...'\n",
      "Vector search found 5 results for query: 'Data drift detection methods'\n",
      "  Query 4/10: 'How to customize embedding drift detection?...'\n",
      "Vector search found 5 results for query: 'How to customize embedding drift detection?'\n",
      "  Query 5/10: 'How to create monitoring dashboards?...'\n",
      "Vector search found 5 results for query: 'How to create monitoring dashboards?'\n",
      "  Query 6/10: 'Dashboard panel types and configuration...'\n",
      "Vector search found 5 results for query: 'Dashboard panel types and configuration'\n",
      "  Query 7/10: 'How to generate reports in Evidently?...'\n",
      "Vector search found 5 results for query: 'How to generate reports in Evidently?'\n",
      "  Query 8/10: 'Generate synthetic test data...'\n",
      "Vector search found 5 results for query: 'Generate synthetic test data'\n",
      "  Query 9/10: 'Setting up alerts for model monitoring...'\n",
      "Vector search found 5 results for query: 'Setting up alerts for model monitoring'\n",
      "  Query 10/10: 'Understanding data definition and descriptors...'\n",
      "Vector search found 5 results for query: 'Understanding data definition and descriptors'\n",
      "  â€¢ Hit Rate: 100.00%\n",
      "  â€¢ MRR: 0.695\n",
      "  â€¢ Precision@5: 48.00%\n",
      "\n",
      "ðŸ“Š Evaluating HYBRID search...\n",
      "Starting evaluation with 10 test queries...\n",
      "  Query 1/10: 'What components are required in a test dataset to ...'\n",
      "Hybrid search found 5 results for query: 'What components are required in a test dataset to evaluate AI?'\n",
      "  Query 2/10: 'How to run evaluations in Evidently?...'\n",
      "Hybrid search found 5 results for query: 'How to run evaluations in Evidently?'\n",
      "  Query 3/10: 'Data drift detection methods...'\n",
      "Hybrid search found 5 results for query: 'Data drift detection methods'\n",
      "  Query 4/10: 'How to customize embedding drift detection?...'\n",
      "Hybrid search found 5 results for query: 'How to customize embedding drift detection?'\n",
      "  Query 5/10: 'How to create monitoring dashboards?...'\n",
      "Hybrid search found 5 results for query: 'How to create monitoring dashboards?'\n",
      "  Query 6/10: 'Dashboard panel types and configuration...'\n",
      "Hybrid search found 5 results for query: 'Dashboard panel types and configuration'\n",
      "  Query 7/10: 'How to generate reports in Evidently?...'\n",
      "Hybrid search found 5 results for query: 'How to generate reports in Evidently?'\n",
      "  Query 8/10: 'Generate synthetic test data...'\n",
      "Hybrid search found 5 results for query: 'Generate synthetic test data'\n",
      "  Query 9/10: 'Setting up alerts for model monitoring...'\n",
      "Hybrid search found 5 results for query: 'Setting up alerts for model monitoring'\n",
      "  Query 10/10: 'Understanding data definition and descriptors...'\n",
      "Hybrid search found 5 results for query: 'Understanding data definition and descriptors'\n",
      "  â€¢ Hit Rate: 90.00%\n",
      "  â€¢ MRR: 0.670\n",
      "  â€¢ Precision@5: 50.00%\n",
      "\n",
      " BEST METHODS:\n",
      "  â€¢ Best Hit Rate: VECTOR\n",
      "  â€¢ Best MRR: VECTOR\n",
      "  â€¢ Best Precision: HYBRID\n",
      "\n",
      "ðŸ“ˆ Hybrid vs Text Search Improvement:\n",
      "  â€¢ Hit Rate: +30.0%\n",
      "  â€¢ MRR: +0.120\n"
     ]
    }
   ],
   "source": [
    "def compare_search_methods(test_queries):\n",
    "    \"\"\"\n",
    "    Compare text, vector, and hybrid search performance.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" COMPARING SEARCH METHODS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    methods = {\n",
    "        'text': lambda q: text_search(q),\n",
    "        'vector': lambda q: vector_search(q),\n",
    "        'hybrid': lambda q: hybrid_search(q, alpha=0.5)\n",
    "    }\n",
    "    \n",
    "    comparison_results = {}\n",
    "    \n",
    "    for method_name, search_fn in methods.items():\n",
    "        print(f\"\\nðŸ“Š Evaluating {method_name.upper()} search...\")\n",
    "        \n",
    "        eval_results = evaluate_search_quality(\n",
    "            search_function=search_fn,\n",
    "            test_queries=test_queries,\n",
    "            num_results=5,\n",
    "            log_results=False  # Don't log intermediate results\n",
    "        )\n",
    "        \n",
    "        metrics = eval_results['metrics']\n",
    "        comparison_results[method_name] = {\n",
    "            'hit_rate': metrics['hit_rate'],\n",
    "            'mrr': metrics['mean_reciprocal_rank'],\n",
    "            'precision': metrics['mean_precision_at_k'],\n",
    "            'detailed_results': eval_results['detailed_results']\n",
    "        }\n",
    "        \n",
    "        print(f\"  â€¢ Hit Rate: {metrics['hit_rate']:.2%}\")\n",
    "        print(f\"  â€¢ MRR: {metrics['mean_reciprocal_rank']:.3f}\")\n",
    "        print(f\"  â€¢ Precision@5: {metrics['mean_precision_at_k']:.2%}\")\n",
    "    \n",
    "    # Find best method for each metric\n",
    "    print(\"\\n BEST METHODS:\")\n",
    "    print(f\"  â€¢ Best Hit Rate: {max(comparison_results.items(), key=lambda x: x[1]['hit_rate'])[0].upper()}\")\n",
    "    print(f\"  â€¢ Best MRR: {max(comparison_results.items(), key=lambda x: x[1]['mrr'])[0].upper()}\")\n",
    "    print(f\"  â€¢ Best Precision: {max(comparison_results.items(), key=lambda x: x[1]['precision'])[0].upper()}\")\n",
    "    \n",
    "    # Show improvement from text to hybrid\n",
    "    if 'text' in comparison_results and 'hybrid' in comparison_results:\n",
    "        hit_rate_improvement = comparison_results['hybrid']['hit_rate'] - comparison_results['text']['hit_rate']\n",
    "        mrr_improvement = comparison_results['hybrid']['mrr'] - comparison_results['text']['mrr']\n",
    "        \n",
    "        print(f\"\\nðŸ“ˆ Hybrid vs Text Search Improvement:\")\n",
    "        print(f\"  â€¢ Hit Rate: {hit_rate_improvement:+.1%}\")\n",
    "        print(f\"  â€¢ MRR: {mrr_improvement:+.3f}\")\n",
    "    \n",
    "    return comparison_results\n",
    "\n",
    "# Run comparison\n",
    "comparison = compare_search_methods(test_queries)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "62c5deb3-85c3-463f-af57-6495d59bf7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " OPTIMIZING HYBRID SEARCH ALPHA\n",
      "============================================================\n",
      "\n",
      "ðŸ”§ Testing alpha=0.00\n",
      "Starting evaluation with 10 test queries...\n",
      "  Query 1/10: 'What components are required in a test dataset to ...'\n",
      "Hybrid search found 5 results for query: 'What components are required in a test dataset to evaluate AI?'\n",
      "  Query 2/10: 'How to run evaluations in Evidently?...'\n",
      "Hybrid search found 5 results for query: 'How to run evaluations in Evidently?'\n",
      "  Query 3/10: 'Data drift detection methods...'\n",
      "Hybrid search found 5 results for query: 'Data drift detection methods'\n",
      "  Query 4/10: 'How to customize embedding drift detection?...'\n",
      "Hybrid search found 5 results for query: 'How to customize embedding drift detection?'\n",
      "  Query 5/10: 'How to create monitoring dashboards?...'\n",
      "Hybrid search found 5 results for query: 'How to create monitoring dashboards?'\n",
      "  Query 6/10: 'Dashboard panel types and configuration...'\n",
      "Hybrid search found 5 results for query: 'Dashboard panel types and configuration'\n",
      "  Query 7/10: 'How to generate reports in Evidently?...'\n",
      "Hybrid search found 5 results for query: 'How to generate reports in Evidently?'\n",
      "  Query 8/10: 'Generate synthetic test data...'\n",
      "Hybrid search found 5 results for query: 'Generate synthetic test data'\n",
      "  Query 9/10: 'Setting up alerts for model monitoring...'\n",
      "Hybrid search found 5 results for query: 'Setting up alerts for model monitoring'\n",
      "  Query 10/10: 'Understanding data definition and descriptors...'\n",
      "Hybrid search found 5 results for query: 'Understanding data definition and descriptors'\n",
      "  â€¢ Hit Rate: 100.00%\n",
      "  â€¢ MRR: 0.695\n",
      "\n",
      "ðŸ”§ Testing alpha=0.25\n",
      "Starting evaluation with 10 test queries...\n",
      "  Query 1/10: 'What components are required in a test dataset to ...'\n",
      "Hybrid search found 5 results for query: 'What components are required in a test dataset to evaluate AI?'\n",
      "  Query 2/10: 'How to run evaluations in Evidently?...'\n",
      "Hybrid search found 5 results for query: 'How to run evaluations in Evidently?'\n",
      "  Query 3/10: 'Data drift detection methods...'\n",
      "Hybrid search found 5 results for query: 'Data drift detection methods'\n",
      "  Query 4/10: 'How to customize embedding drift detection?...'\n",
      "Hybrid search found 5 results for query: 'How to customize embedding drift detection?'\n",
      "  Query 5/10: 'How to create monitoring dashboards?...'\n",
      "Hybrid search found 5 results for query: 'How to create monitoring dashboards?'\n",
      "  Query 6/10: 'Dashboard panel types and configuration...'\n",
      "Hybrid search found 5 results for query: 'Dashboard panel types and configuration'\n",
      "  Query 7/10: 'How to generate reports in Evidently?...'\n",
      "Hybrid search found 5 results for query: 'How to generate reports in Evidently?'\n",
      "  Query 8/10: 'Generate synthetic test data...'\n",
      "Hybrid search found 5 results for query: 'Generate synthetic test data'\n",
      "  Query 9/10: 'Setting up alerts for model monitoring...'\n",
      "Hybrid search found 5 results for query: 'Setting up alerts for model monitoring'\n",
      "  Query 10/10: 'Understanding data definition and descriptors...'\n",
      "Hybrid search found 5 results for query: 'Understanding data definition and descriptors'\n",
      "  â€¢ Hit Rate: 100.00%\n",
      "  â€¢ MRR: 0.745\n",
      "\n",
      "ðŸ”§ Testing alpha=0.50\n",
      "Starting evaluation with 10 test queries...\n",
      "  Query 1/10: 'What components are required in a test dataset to ...'\n",
      "Hybrid search found 5 results for query: 'What components are required in a test dataset to evaluate AI?'\n",
      "  Query 2/10: 'How to run evaluations in Evidently?...'\n",
      "Hybrid search found 5 results for query: 'How to run evaluations in Evidently?'\n",
      "  Query 3/10: 'Data drift detection methods...'\n",
      "Hybrid search found 5 results for query: 'Data drift detection methods'\n",
      "  Query 4/10: 'How to customize embedding drift detection?...'\n",
      "Hybrid search found 5 results for query: 'How to customize embedding drift detection?'\n",
      "  Query 5/10: 'How to create monitoring dashboards?...'\n",
      "Hybrid search found 5 results for query: 'How to create monitoring dashboards?'\n",
      "  Query 6/10: 'Dashboard panel types and configuration...'\n",
      "Hybrid search found 5 results for query: 'Dashboard panel types and configuration'\n",
      "  Query 7/10: 'How to generate reports in Evidently?...'\n",
      "Hybrid search found 5 results for query: 'How to generate reports in Evidently?'\n",
      "  Query 8/10: 'Generate synthetic test data...'\n",
      "Hybrid search found 5 results for query: 'Generate synthetic test data'\n",
      "  Query 9/10: 'Setting up alerts for model monitoring...'\n",
      "Hybrid search found 5 results for query: 'Setting up alerts for model monitoring'\n",
      "  Query 10/10: 'Understanding data definition and descriptors...'\n",
      "Hybrid search found 5 results for query: 'Understanding data definition and descriptors'\n",
      "  â€¢ Hit Rate: 90.00%\n",
      "  â€¢ MRR: 0.670\n",
      "\n",
      "ðŸ”§ Testing alpha=0.75\n",
      "Starting evaluation with 10 test queries...\n",
      "  Query 1/10: 'What components are required in a test dataset to ...'\n",
      "Hybrid search found 5 results for query: 'What components are required in a test dataset to evaluate AI?'\n",
      "  Query 2/10: 'How to run evaluations in Evidently?...'\n",
      "Hybrid search found 5 results for query: 'How to run evaluations in Evidently?'\n",
      "  Query 3/10: 'Data drift detection methods...'\n",
      "Hybrid search found 5 results for query: 'Data drift detection methods'\n",
      "  Query 4/10: 'How to customize embedding drift detection?...'\n",
      "Hybrid search found 5 results for query: 'How to customize embedding drift detection?'\n",
      "  Query 5/10: 'How to create monitoring dashboards?...'\n",
      "Hybrid search found 5 results for query: 'How to create monitoring dashboards?'\n",
      "  Query 6/10: 'Dashboard panel types and configuration...'\n",
      "Hybrid search found 5 results for query: 'Dashboard panel types and configuration'\n",
      "  Query 7/10: 'How to generate reports in Evidently?...'\n",
      "Hybrid search found 5 results for query: 'How to generate reports in Evidently?'\n",
      "  Query 8/10: 'Generate synthetic test data...'\n",
      "Hybrid search found 5 results for query: 'Generate synthetic test data'\n",
      "  Query 9/10: 'Setting up alerts for model monitoring...'\n",
      "Hybrid search found 5 results for query: 'Setting up alerts for model monitoring'\n",
      "  Query 10/10: 'Understanding data definition and descriptors...'\n",
      "Hybrid search found 5 results for query: 'Understanding data definition and descriptors'\n",
      "  â€¢ Hit Rate: 70.00%\n",
      "  â€¢ MRR: 0.583\n",
      "\n",
      "ðŸ”§ Testing alpha=1.00\n",
      "Starting evaluation with 10 test queries...\n",
      "  Query 1/10: 'What components are required in a test dataset to ...'\n",
      "Hybrid search found 5 results for query: 'What components are required in a test dataset to evaluate AI?'\n",
      "  Query 2/10: 'How to run evaluations in Evidently?...'\n",
      "Hybrid search found 5 results for query: 'How to run evaluations in Evidently?'\n",
      "  Query 3/10: 'Data drift detection methods...'\n",
      "Hybrid search found 5 results for query: 'Data drift detection methods'\n",
      "  Query 4/10: 'How to customize embedding drift detection?...'\n",
      "Hybrid search found 5 results for query: 'How to customize embedding drift detection?'\n",
      "  Query 5/10: 'How to create monitoring dashboards?...'\n",
      "Hybrid search found 5 results for query: 'How to create monitoring dashboards?'\n",
      "  Query 6/10: 'Dashboard panel types and configuration...'\n",
      "Hybrid search found 5 results for query: 'Dashboard panel types and configuration'\n",
      "  Query 7/10: 'How to generate reports in Evidently?...'\n",
      "Hybrid search found 5 results for query: 'How to generate reports in Evidently?'\n",
      "  Query 8/10: 'Generate synthetic test data...'\n",
      "Hybrid search found 5 results for query: 'Generate synthetic test data'\n",
      "  Query 9/10: 'Setting up alerts for model monitoring...'\n",
      "Hybrid search found 5 results for query: 'Setting up alerts for model monitoring'\n",
      "  Query 10/10: 'Understanding data definition and descriptors...'\n",
      "Hybrid search found 5 results for query: 'Understanding data definition and descriptors'\n",
      "  â€¢ Hit Rate: 60.00%\n",
      "  â€¢ MRR: 0.550\n",
      "\n",
      "ðŸŽ¯ OPTIMAL ALPHA VALUES:\n",
      "  â€¢ Best for MRR: Î±=0.25 (MRR=0.745)\n",
      "  â€¢ Best for Hit Rate: Î±=0.00 (Hit Rate=100.00%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def optimize_hybrid_alpha(test_queries, alphas=[0.0, 0.25, 0.5, 0.75, 1.0]):\n",
    "    \"\"\"\n",
    "    Find the optimal alpha value for hybrid search.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" OPTIMIZING HYBRID SEARCH ALPHA\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        print(f\"\\nðŸ”§ Testing alpha={alpha:.2f}\")\n",
    "        \n",
    "        hybrid_fn = lambda q: hybrid_search(q, alpha=alpha)\n",
    "        \n",
    "        eval_results = evaluate_search_quality(\n",
    "            search_function=hybrid_fn,\n",
    "            test_queries=test_queries,\n",
    "            num_results=5,\n",
    "            log_results=False\n",
    "        )\n",
    "        \n",
    "        metrics = eval_results['metrics']\n",
    "        results.append({\n",
    "            'alpha': alpha,\n",
    "            'hit_rate': metrics['hit_rate'],\n",
    "            'mrr': metrics['mean_reciprocal_rank'],\n",
    "            'precision': metrics['mean_precision_at_k']\n",
    "        })\n",
    "        \n",
    "        print(f\"  â€¢ Hit Rate: {metrics['hit_rate']:.2%}\")\n",
    "        print(f\"  â€¢ MRR: {metrics['mean_reciprocal_rank']:.3f}\")\n",
    "    \n",
    "    # Find optimal alpha\n",
    "    best_by_mrr = max(results, key=lambda x: x['mrr'])\n",
    "    best_by_hit_rate = max(results, key=lambda x: x['hit_rate'])\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ OPTIMAL ALPHA VALUES:\")\n",
    "    print(f\"  â€¢ Best for MRR: Î±={best_by_mrr['alpha']:.2f} (MRR={best_by_mrr['mrr']:.3f})\")\n",
    "    print(f\"  â€¢ Best for Hit Rate: Î±={best_by_hit_rate['alpha']:.2f} (Hit Rate={best_by_hit_rate['hit_rate']:.2%})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Optimize alpha\n",
    "alpha_results = optimize_hybrid_alpha(test_queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f5e423a-90e8-4805-bc4a-f65a8e23bfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸš€ FINAL EVALUATION WITH OPTIMIZED HYBRID SEARCH\n",
      "============================================================\n",
      "Starting evaluation with 10 test queries...\n",
      "  Query 1/10: 'What components are required in a test dataset to ...'\n",
      "Hybrid search found 5 results for query: 'What components are required in a test dataset to evaluate AI?'\n",
      "  Query 2/10: 'How to run evaluations in Evidently?...'\n",
      "Hybrid search found 5 results for query: 'How to run evaluations in Evidently?'\n",
      "  Query 3/10: 'Data drift detection methods...'\n",
      "Hybrid search found 5 results for query: 'Data drift detection methods'\n",
      "  Query 4/10: 'How to customize embedding drift detection?...'\n",
      "Hybrid search found 5 results for query: 'How to customize embedding drift detection?'\n",
      "  Query 5/10: 'How to create monitoring dashboards?...'\n",
      "Hybrid search found 5 results for query: 'How to create monitoring dashboards?'\n",
      "  Query 6/10: 'Dashboard panel types and configuration...'\n",
      "Hybrid search found 5 results for query: 'Dashboard panel types and configuration'\n",
      "  Query 7/10: 'How to generate reports in Evidently?...'\n",
      "Hybrid search found 5 results for query: 'How to generate reports in Evidently?'\n",
      "  Query 8/10: 'Generate synthetic test data...'\n",
      "Hybrid search found 5 results for query: 'Generate synthetic test data'\n",
      "  Query 9/10: 'Setting up alerts for model monitoring...'\n",
      "Hybrid search found 5 results for query: 'Setting up alerts for model monitoring'\n",
      "  Query 10/10: 'Understanding data definition and descriptors...'\n",
      "Hybrid search found 5 results for query: 'Understanding data definition and descriptors'\n",
      "ðŸ“ Evaluation results saved to: logs/search_evaluation_20250930_222752_8ea0c6.json\n",
      "\n",
      "============================================================\n",
      "ðŸ“Š EVALUATION REPORT\n",
      "============================================================\n",
      "Timestamp: 2025-09-30T22:27:52.570849\n",
      "Number of queries: 10\n",
      "Results per query: 5\n",
      "\n",
      "ðŸ“ˆ AGGREGATE METRICS:\n",
      "  â€¢ Hit Rate: 90.00%\n",
      "  â€¢ Mean Reciprocal Rank (MRR): 0.670\n",
      "  â€¢ Mean Precision@5: 50.00%\n",
      "\n",
      "ðŸ” QUERY-LEVEL RESULTS:\n",
      "\n",
      "  Query 1: \"What components are required in a test dataset to ...\"\n",
      "    â€¢ Hit: âœ…\n",
      "    â€¢ MRR: 0.500\n",
      "    â€¢ Precision: 40.00%\n",
      "    â€¢ First relevant at rank: 2\n",
      "\n",
      "  Query 2: \"How to run evaluations in Evidently?...\"\n",
      "    â€¢ Hit: âŒ\n",
      "    â€¢ MRR: 0.000\n",
      "    â€¢ Precision: 0.00%\n",
      "\n",
      "  Query 3: \"Data drift detection methods...\"\n",
      "    â€¢ Hit: âœ…\n",
      "    â€¢ MRR: 0.500\n",
      "    â€¢ Precision: 40.00%\n",
      "    â€¢ First relevant at rank: 2\n",
      "\n",
      "  Query 4: \"How to customize embedding drift detection?...\"\n",
      "    â€¢ Hit: âœ…\n",
      "    â€¢ MRR: 1.000\n",
      "    â€¢ Precision: 100.00%\n",
      "    â€¢ First relevant at rank: 1\n",
      "\n",
      "  Query 5: \"How to create monitoring dashboards?...\"\n",
      "    â€¢ Hit: âœ…\n",
      "    â€¢ MRR: 0.200\n",
      "    â€¢ Precision: 20.00%\n",
      "    â€¢ First relevant at rank: 5\n",
      "\n",
      "  ... and 5 more queries\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸš€ FINAL EVALUATION WITH OPTIMIZED HYBRID SEARCH\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use the best alpha from optimization (you can adjust based on results)\n",
    "best_alpha = 0.5  # Adjust based on optimization results\n",
    "\n",
    "final_search = lambda q: hybrid_search(q, alpha=best_alpha)\n",
    "\n",
    "final_evaluation = evaluate_search_quality(\n",
    "    search_function=final_search,\n",
    "    test_queries=test_queries,\n",
    "    num_results=5,\n",
    "    log_results=True\n",
    ")\n",
    "\n",
    "print_evaluation_report(final_evaluation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7ddf10-5dfc-44af-9d06-4795b7edce2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae21ed0c-9c9d-46fb-a830-86ea3bb1a37f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3570063-bfbc-4100-b6f3-39ed147f333c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5f1987-91fd-4cfb-8aca-34ea75e33e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06c3534-abea-4077-98d1-e271e144e169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv)",
   "language": "python",
   "name": "uv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
