{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b50e3e2-fef7-48d5-ab02-d34081fe15ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "import logging\n",
    "import asyncio\n",
    "from tqdm import tqdm\n",
    "from minsearch import Index\n",
    "from typing import List, Any, Dict, Tuple, Optional\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pydantic_ai import Agent\n",
    "import json\n",
    "import secrets\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pydantic_ai.messages import ModelMessagesTypeAdapter\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e4900d3-1f53-4bde-910c-591345b732b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_repo_data(repo_owner, repo_name, branch=\"main\"):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    Yields one document (dict) at a time to avoid loading everything into memory.\n",
    "\n",
    "    Args:\n",
    "        repo_owner (str): GitHub username or organization\n",
    "        repo_name (str): Repository name\n",
    "        branch (str): Branch name (default: main)\n",
    "    \"\"\"\n",
    "    url = f\"https://codeload.github.com/{repo_owner}/{repo_name}/zip/refs/heads/{branch}\"\n",
    "    resp = requests.get(url)\n",
    "\n",
    "    if resp.status_code == 404 and branch == \"main\":\n",
    "        # Try fallback to master\n",
    "        return read_repo_data(repo_owner, repo_name, branch=\"master\")\n",
    "\n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: HTTP {resp.status_code}\")\n",
    "\n",
    "    with zipfile.ZipFile(io.BytesIO(resp.content)) as zf:\n",
    "        for file_info in zf.infolist():\n",
    "            filename = file_info.filename\n",
    "            if not filename.lower().endswith((\".md\", \".mdx\")):\n",
    "                continue\n",
    "            try:\n",
    "                with zf.open(file_info) as f_in:\n",
    "                    content = f_in.read().decode(\"utf-8\", errors=\"replace\")\n",
    "                    post = frontmatter.loads(content)\n",
    "                    data = post.to_dict()\n",
    "                    data.update({\n",
    "                        \"filename\": filename,\n",
    "                        \"repo\": repo_name,\n",
    "                        \"owner\": repo_owner,\n",
    "                        \"branch\": branch\n",
    "                    })\n",
    "                    yield data\n",
    "            except Exception as e:\n",
    "                logging.warning(\"Error processing %s: %s\", filename, e)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a7494ff-4c51-4680-8b4b-0751139ee23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq, size, step):\n",
    "    \"\"\"Yield overlapping chunks from a long string.\"\"\"\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "    n = len(seq)\n",
    "    for i in range(0, n, step):\n",
    "        yield {\"start\": i, \"chunk\": seq[i:i+size]}\n",
    "        if i + size >= n:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a79dc7e-7341-4ca0-b103-0f9916fb5283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 95it [00:00, 115.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 575 chunks. Building index...\n",
      "Text indexing complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evidently_chunks = []\n",
    "\n",
    "for doc in tqdm(read_repo_data(\"evidentlyai\", \"docs\"), desc=\"Processing files\"):\n",
    "    doc_copy = doc.copy()\n",
    "    content = doc_copy.pop(\"content\", \"\")\n",
    "    for chunk in sliding_window(content, size=2000, step=1000):\n",
    "        chunk.update(doc_copy)\n",
    "        evidently_chunks.append(chunk)\n",
    "\n",
    "print(f\"Collected {len(evidently_chunks)} chunks. Building index...\")\n",
    "\n",
    "# Build text search index\n",
    "index = Index(\n",
    "    text_fields=[\"chunk\", \"title\", \"description\", \"filename\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "index.fit(evidently_chunks)\n",
    "\n",
    "print(\"Text indexing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46836ed8-7782-4545-a40d-c86a51222df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorSearch:\n",
    "    \"\"\"\n",
    "    Simple vector search implementation using cosine similarity.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.embeddings = None\n",
    "        self.documents = None\n",
    "    \n",
    "    def fit(self, embeddings: np.ndarray, documents: List[Dict]):\n",
    "        \"\"\"\n",
    "        Store embeddings and associated documents.\n",
    "        \"\"\"\n",
    "        self.embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)  # Normalize\n",
    "        self.documents = documents\n",
    "    \n",
    "    def search(self, query_embedding: np.ndarray, num_results: int = 5) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search for most similar documents using cosine similarity.\n",
    "        \"\"\"\n",
    "        if self.embeddings is None:\n",
    "            return []\n",
    "        \n",
    "        # Normalize query embedding\n",
    "        query_norm = query_embedding / np.linalg.norm(query_embedding)\n",
    "        \n",
    "        # Calculate cosine similarities\n",
    "        similarities = np.dot(self.embeddings, query_norm)\n",
    "        \n",
    "        # Get top k indices\n",
    "        top_indices = np.argsort(similarities)[-num_results:][::-1]\n",
    "        \n",
    "        # Return documents with similarity scores\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            doc = self.documents[idx].copy()\n",
    "            doc['similarity_score'] = float(similarities[idx])\n",
    "            results.append(doc)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "178d1699-ec0c-4073-987c-a47fd1a68bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings for semantic search...\n",
      "Loading embedding model...\n",
      "Encoding document chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 575/575 [05:16<00:00,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector indexing complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating embeddings for semantic search...\")\n",
    "\n",
    "# Initialize an empty list to store embeddings for each chunk\n",
    "evidently_embeddings = []\n",
    "\n",
    "# Load a pre-trained sentence transformer model for creating embeddings\n",
    "print(\"Loading embedding model...\")\n",
    "embedding_model = SentenceTransformer('multi-qa-distilbert-cos-v1')\n",
    "\n",
    "# Loop through each document chunk in evidently_chunks\n",
    "print(\"Encoding document chunks...\")\n",
    "for d in tqdm(evidently_chunks, desc=\"Creating embeddings\"):\n",
    "    # Create a combined text for better context\n",
    "    text_to_encode = d['chunk']\n",
    "    if 'title' in d and d['title']:\n",
    "        text_to_encode = f\"{d['title']}. {text_to_encode}\"\n",
    "    if 'filename' in d:\n",
    "        # Extract meaningful parts from filename\n",
    "        filename_parts = d['filename'].replace('/', ' ').replace('_', ' ').replace('.mdx', '').replace('.md', '')\n",
    "        text_to_encode = f\"{filename_parts}. {text_to_encode}\"\n",
    "    \n",
    "    # Encode the enhanced text into a vector (embedding)\n",
    "    v = embedding_model.encode(text_to_encode, show_progress_bar=False)\n",
    "    \n",
    "    # Append the embedding to the list\n",
    "    evidently_embeddings.append(v)\n",
    "\n",
    "# Convert the list of embeddings into a NumPy array\n",
    "evidently_embeddings = np.array(evidently_embeddings)\n",
    "\n",
    "# Initialize and fit vector search index\n",
    "evidently_vindex = VectorSearch()\n",
    "evidently_vindex.fit(evidently_embeddings, evidently_chunks)\n",
    "\n",
    "print(\"Vector indexing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a45f43ee-427f-4632-8aa5-8d7ee7f7fc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection Error: Error code: 429 - {'error': {'message': 'Provider returned error', 'code': 429, 'metadata': {'raw': 'deepseek/deepseek-r1:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations', 'provider_name': 'Chutes'}}, 'user_id': 'user_33P07DTlgylC1rY1SOCrqh6snaU'}\n",
      "Make sure you have OPENROUTER_API_KEY in your .env file\n"
     ]
    }
   ],
   "source": [
    "openai_client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\")\n",
    ")\n",
    "\n",
    "# Test the connection\n",
    "try:\n",
    "    test_response = openai_client.chat.completions.create(\n",
    "        model=\"deepseek/deepseek-r1:free\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Say 'Connection successful' if you can read this.\"}],\n",
    "        max_tokens=50\n",
    "    )\n",
    "    print(f\"OpenRouter Connection Test: {test_response.choices[0].message.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Connection Error: {e}\")\n",
    "    print(\"Make sure you have OPENROUTER_API_KEY in your .env file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2c87751-6e85-4cec-aee6-49fc0bc9f46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Testing search methods:\n",
      "Text search found 5 results for query: 'test dataset'\n",
      "Vector search found 5 results for query: 'test dataset'\n",
      "Hybrid search found 5 results for query: 'test dataset'\n"
     ]
    }
   ],
   "source": [
    "def text_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Perform a text-based search on the FAQ index.\n",
    "    \"\"\"\n",
    "    results = index.search(query, num_results=5)\n",
    "    print(f\"Text search found {len(results)} results for query: '{query}'\")\n",
    "    return results\n",
    "\n",
    "def vector_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Perform semantic vector-based search.\n",
    "    \"\"\"\n",
    "    # Encode the query into a vector using the embedding model\n",
    "    q = embedding_model.encode(query)\n",
    "    # Search the vector index for the top 5 most similar chunks\n",
    "    results = evidently_vindex.search(q, num_results=5)\n",
    "    print(f\"Vector search found {len(results)} results for query: '{query}'\")\n",
    "    return results\n",
    "\n",
    "def hybrid_search(query: str, alpha: float = 0.5, num_results: int = 5) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Perform hybrid search combining text and vector search.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        alpha: Weight for text search (1-alpha for vector search)\n",
    "        num_results: Number of results to return\n",
    "    \"\"\"\n",
    "    # Get results from both search methods\n",
    "    text_results = index.search(query, num_results=num_results*2)\n",
    "    \n",
    "    q_embedding = embedding_model.encode(query)\n",
    "    vector_results = evidently_vindex.search(q_embedding, num_results=num_results*2)\n",
    "    \n",
    "    # Create a scoring dictionary\n",
    "    doc_scores = {}\n",
    "    \n",
    "    # Add text search scores\n",
    "    for i, doc in enumerate(text_results):\n",
    "        doc_key = doc.get('filename', '') + str(doc.get('start', 0))\n",
    "        # Use inverse rank as score\n",
    "        text_score = 1.0 / (i + 1)\n",
    "        doc_scores[doc_key] = {\n",
    "            'doc': doc,\n",
    "            'text_score': text_score * alpha,\n",
    "            'vector_score': 0\n",
    "        }\n",
    "    \n",
    "    # Add vector search scores\n",
    "    for doc in vector_results:\n",
    "        doc_key = doc.get('filename', '') + str(doc.get('start', 0))\n",
    "        vector_score = doc.get('similarity_score', 0) * (1 - alpha)\n",
    "        \n",
    "        if doc_key in doc_scores:\n",
    "            doc_scores[doc_key]['vector_score'] = vector_score\n",
    "        else:\n",
    "            doc_scores[doc_key] = {\n",
    "                'doc': doc,\n",
    "                'text_score': 0,\n",
    "                'vector_score': vector_score\n",
    "            }\n",
    "    \n",
    "    # Calculate combined scores\n",
    "    for key in doc_scores:\n",
    "        doc_scores[key]['combined_score'] = doc_scores[key]['text_score'] + doc_scores[key]['vector_score']\n",
    "    \n",
    "    # Sort by combined score and return top results\n",
    "    sorted_docs = sorted(doc_scores.values(), key=lambda x: x['combined_score'], reverse=True)\n",
    "    results = [item['doc'] for item in sorted_docs[:num_results]]\n",
    "    \n",
    "    print(f\"Hybrid search found {len(results)} results for query: '{query}'\")\n",
    "    return results\n",
    "\n",
    "# Test all search methods\n",
    "test_query = \"test dataset\"\n",
    "print(\"\\n Testing search methods:\")\n",
    "text_results = text_search(test_query)\n",
    "vector_results = vector_search(test_query)\n",
    "hybrid_results = hybrid_search(test_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6107d19-bc52-41d6-95aa-8a82f073622a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â“ Question: What components are required in a test dataset to evaluate AI?\n",
      "Thinking (using hybrid search + LLM approach)...\n",
      "Hybrid search found 5 results for query: 'What components are required in a test dataset to evaluate AI?'\n",
      "\n",
      " Answer:\n",
      "To evaluate AI systems like a RAG using Evidently, a test dataset should include:\n",
      "- **User-like questions** generated from your knowledge base\n",
      "- Corresponding **ground truth answers** extracted from the data source\n",
      "- (Optional) **Context snippets** used to generate the answers\n",
      "\n",
      "These components are automatically created when generating RAG test datasets via Evidently UI by uploading your knowledge base files. The system produces question-answer pairs that reflect what your AI *should* know, serving as validation benchmarks.\n"
     ]
    }
   ],
   "source": [
    "def answer_question_manual(question: str, search_method: str = 'hybrid') -> str:\n",
    "    \"\"\"\n",
    "    Answer a question by searching and then using the LLM.\n",
    "    \n",
    "    Args:\n",
    "        question: The question to answer\n",
    "        search_method: 'text', 'vector', or 'hybrid'\n",
    "    \"\"\"\n",
    "    # Select search method\n",
    "    if search_method == 'text':\n",
    "        search_results = text_search(question)\n",
    "    elif search_method == 'vector':\n",
    "        search_results = vector_search(question)\n",
    "    else:  # hybrid\n",
    "        search_results = hybrid_search(question)\n",
    "\n",
    "    # Format the search results as context\n",
    "    context = \"\\n\\n---\\n\\n\".join([\n",
    "        f\"Result {i+1} (from {result.get('filename', 'unknown')}):\\n{result.get('chunk', '')}\"\n",
    "        for i, result in enumerate(search_results)\n",
    "    ])\n",
    "\n",
    "    # Create the prompt with the context\n",
    "    prompt = f\"\"\"You are an expert assistant that answers questions about the Evidently project \n",
    "(https://github.com/evidentlyai/evidently) using ONLY the information provided in the context below.\n",
    "\n",
    "Context from Evidently documentation:\n",
    "{context}\n",
    "\n",
    "User question: {question}\n",
    "\n",
    "Instructions:\n",
    "- Answer based ONLY on the provided context\n",
    "- Be concise and clear\n",
    "- If the answer is not in the context, say \"I could not find this information in the Evidently documentation\"\n",
    "- Do not invent features or functionality\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"deepseek/deepseek-r1:free\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions about Evidently.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=1000\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error getting answer: {e}\"\n",
    "\n",
    "# Test question answering\n",
    "question = \"What components are required in a test dataset to evaluate AI?\"\n",
    "print(f\"\\nâ“ Question: {question}\")\n",
    "print(\"Thinking (using hybrid search + LLM approach)...\")\n",
    "answer = answer_question_manual(question, search_method='hybrid')\n",
    "print(f\"\\n Answer:\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a50838ff-e46c-4b8a-ad4e-d0b226c31fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = Path(os.getenv('LOGS_DIRECTORY', 'logs'))\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def log_entry(agent, messages, source=\"user\"):\n",
    "    tools = []\n",
    "    for ts in agent.toolsets:\n",
    "        tools.extend(ts.tools.keys())\n",
    "    dict_messages = ModelMessagesTypeAdapter.dump_python(messages)\n",
    "    return {\n",
    "        \"agent_name\": agent.name,\n",
    "        \"system_prompt\": agent._instructions,\n",
    "        \"provider\": agent.model.system,\n",
    "        \"model\": agent.model.model_name,\n",
    "        \"tools\": tools,\n",
    "        \"messages\": dict_messages,\n",
    "        \"source\": source\n",
    "    }\n",
    "\n",
    "def serializer(obj):\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "def log_interaction_to_file(agent, messages, source='user'):\n",
    "    entry = log_entry(agent, messages, source)\n",
    "    ts = entry['messages'][-1]['timestamp']\n",
    "    ts_str = ts.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    rand_hex = secrets.token_hex(3)\n",
    "    filename = f\"{agent.name}_{ts_str}_{rand_hex}.json\"\n",
    "    filepath = LOG_DIR / filename\n",
    "    with filepath.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(entry, f_out, indent=2, default=serializer)\n",
    "    return filepath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b9a816e-c018-4d5b-95dc-a40596c03b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_search_quality(\n",
    "    search_function, \n",
    "    test_queries: List[Tuple[str, List[str]]], \n",
    "    num_results: int = 5,\n",
    "    log_results: bool = True\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate search quality using hit rate and MRR metrics.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    timestamp = datetime.now()\n",
    "\n",
    "    print(f\"Starting evaluation with {len(test_queries)} test queries...\")\n",
    "\n",
    "    for idx, (query, expected_docs) in enumerate(test_queries, 1):\n",
    "        print(f\"  Query {idx}/{len(test_queries)}: '{query[:50]}...'\")\n",
    "\n",
    "        try:\n",
    "            # Execute search\n",
    "            if hasattr(search_function, '__self__'):  # Method\n",
    "                search_results = search_function(query, num_results=num_results)\n",
    "            else:  # Function\n",
    "                search_results = search_function(query)[:num_results]\n",
    "\n",
    "            # Extract filenames from results\n",
    "            retrieved_docs = [doc.get('filename', '') for doc in search_results]\n",
    "\n",
    "            # Calculate hit rate (binary: found at least one relevant doc)\n",
    "            relevant_found = any(doc in expected_docs for doc in retrieved_docs)\n",
    "\n",
    "            # Calculate MRR (Mean Reciprocal Rank)\n",
    "            mrr = 0\n",
    "            first_relevant_rank = None\n",
    "            for i, doc in enumerate(retrieved_docs):\n",
    "                if doc in expected_docs:\n",
    "                    mrr = 1 / (i + 1)\n",
    "                    first_relevant_rank = i + 1\n",
    "                    break\n",
    "\n",
    "            # Calculate Precision@k\n",
    "            relevant_in_results = sum(1 for doc in retrieved_docs if doc in expected_docs)\n",
    "            precision_at_k = relevant_in_results / len(retrieved_docs) if retrieved_docs else 0\n",
    "\n",
    "            # Store detailed result\n",
    "            result = {\n",
    "                'query': query,\n",
    "                'expected_docs': expected_docs,\n",
    "                'retrieved_docs': retrieved_docs,\n",
    "                'hit': relevant_found,\n",
    "                'mrr': mrr,\n",
    "                'precision_at_k': precision_at_k,\n",
    "                'first_relevant_rank': first_relevant_rank,\n",
    "                'num_relevant_found': relevant_in_results\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {e}\")\n",
    "            results.append({\n",
    "                'query': query,\n",
    "                'expected_docs': expected_docs,\n",
    "                'retrieved_docs': [],\n",
    "                'hit': False,\n",
    "                'mrr': 0,\n",
    "                'precision_at_k': 0,\n",
    "                'first_relevant_rank': None,\n",
    "                'num_relevant_found': 0,\n",
    "                'error': str(e)\n",
    "            })\n",
    "\n",
    "    # Calculate aggregate metrics\n",
    "    hit_rate = sum(r['hit'] for r in results) / len(results) if results else 0\n",
    "    avg_mrr = sum(r['mrr'] for r in results) / len(results) if results else 0\n",
    "    avg_precision = sum(r['precision_at_k'] for r in results) / len(results) if results else 0\n",
    "\n",
    "    evaluation_summary = {\n",
    "        'timestamp': timestamp.isoformat(),\n",
    "        'num_queries': len(test_queries),\n",
    "        'num_results_per_query': num_results,\n",
    "        'metrics': {\n",
    "            'hit_rate': hit_rate,\n",
    "            'mean_reciprocal_rank': avg_mrr,\n",
    "            'mean_precision_at_k': avg_precision\n",
    "        },\n",
    "        'detailed_results': results\n",
    "    }\n",
    "\n",
    "    # Log results if requested\n",
    "    if log_results:\n",
    "        log_evaluation_results(evaluation_summary)\n",
    "\n",
    "    return evaluation_summary\n",
    "\n",
    "def log_evaluation_results(evaluation_data: Dict[str, Any]) -> Path:\n",
    "    \"\"\"\n",
    "    Log evaluation results to a JSON file.\n",
    "    \"\"\"\n",
    "    ts = datetime.fromisoformat(evaluation_data['timestamp'])\n",
    "    ts_str = ts.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    rand_hex = secrets.token_hex(3)\n",
    "    filename = f\"search_evaluation_{ts_str}_{rand_hex}.json\"\n",
    "    filepath = LOG_DIR / filename\n",
    "    with filepath.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(evaluation_data, f_out, indent=2, default=str)\n",
    "    print(f\"ðŸ“ Evaluation results saved to: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "def print_evaluation_report(evaluation_summary: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Print a formatted evaluation report.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ“Š EVALUATION REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Timestamp: {evaluation_summary['timestamp']}\")\n",
    "    print(f\"Number of queries: {evaluation_summary['num_queries']}\")\n",
    "    print(f\"Results per query: {evaluation_summary['num_results_per_query']}\")\n",
    "\n",
    "    print(\"\\nðŸ“ˆ AGGREGATE METRICS:\")\n",
    "    metrics = evaluation_summary['metrics']\n",
    "    print(f\"  â€¢ Hit Rate: {metrics['hit_rate']:.2%}\")\n",
    "    print(f\"  â€¢ Mean Reciprocal Rank (MRR): {metrics['mean_reciprocal_rank']:.3f}\")\n",
    "    print(f\"  â€¢ Mean Precision@{evaluation_summary['num_results_per_query']}: {metrics['mean_precision_at_k']:.2%}\")\n",
    "\n",
    "    print(\"\\nðŸ” QUERY-LEVEL RESULTS:\")\n",
    "    for i, result in enumerate(evaluation_summary['detailed_results'][:5], 1):\n",
    "        print(f\"\\n  Query {i}: \\\"{result['query'][:50]}...\\\"\")\n",
    "        print(f\"    â€¢ Hit: {'âœ…' if result['hit'] else 'âŒ'}\")\n",
    "        print(f\"    â€¢ MRR: {result['mrr']:.3f}\")\n",
    "        print(f\"    â€¢ Precision: {result['precision_at_k']:.2%}\")\n",
    "        if result['first_relevant_rank']:\n",
    "            print(f\"    â€¢ First relevant at rank: {result['first_relevant_rank']}\")\n",
    "\n",
    "    if len(evaluation_summary['detailed_results']) > 5:\n",
    "        print(f\"\\n  ... and {len(evaluation_summary['detailed_results']) - 5} more queries\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "891dda51-3343-4172-81c7-e6702f2cad6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = [\n",
    "    # Testing and evaluation\n",
    "    (\"What components are required in a test dataset to evaluate AI?\", \n",
    "     [\"docs-main/docs/library/tests.mdx\", \n",
    "      \"docs-main/examples/LLM_regression_testing.mdx\"]),\n",
    "\n",
    "    (\"How to run evaluations in Evidently?\", \n",
    "     [\"docs-main/docs/library/evaluations_overview.mdx\",\n",
    "      \"docs-main/docs/library/tests.mdx\"]),\n",
    "\n",
    "    # Data and metrics\n",
    "    (\"Understanding data definition and descriptors\",\n",
    "     [\"docs-main/docs/library/data_definition.mdx\",\n",
    "      \"docs-main/docs/library/descriptors.mdx\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9e5db7f-5215-47f3-ac35-f5bc283f9342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " COMPARING SEARCH METHODS\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š Evaluating TEXT search...\n",
      "Starting evaluation with 3 test queries...\n",
      "  Query 1/3: 'What components are required in a test dataset to ...'\n",
      "Text search found 5 results for query: 'What components are required in a test dataset to evaluate AI?'\n",
      "  Query 2/3: 'How to run evaluations in Evidently?...'\n",
      "Text search found 5 results for query: 'How to run evaluations in Evidently?'\n",
      "  Query 3/3: 'Understanding data definition and descriptors...'\n",
      "Text search found 5 results for query: 'Understanding data definition and descriptors'\n",
      "  â€¢ Hit Rate: 33.33%\n",
      "  â€¢ MRR: 0.333\n",
      "  â€¢ Precision@5: 33.33%\n",
      "\n",
      "ðŸ“Š Evaluating VECTOR search...\n",
      "Starting evaluation with 3 test queries...\n",
      "  Query 1/3: 'What components are required in a test dataset to ...'\n",
      "Vector search found 5 results for query: 'What components are required in a test dataset to evaluate AI?'\n",
      "  Query 2/3: 'How to run evaluations in Evidently?...'\n",
      "Vector search found 5 results for query: 'How to run evaluations in Evidently?'\n",
      "  Query 3/3: 'Understanding data definition and descriptors...'\n",
      "Vector search found 5 results for query: 'Understanding data definition and descriptors'\n",
      "  â€¢ Hit Rate: 100.00%\n",
      "  â€¢ MRR: 0.567\n",
      "  â€¢ Precision@5: 40.00%\n",
      "\n",
      "ðŸ“Š Evaluating HYBRID search...\n",
      "Starting evaluation with 3 test queries...\n",
      "  Query 1/3: 'What components are required in a test dataset to ...'\n",
      "Hybrid search found 5 results for query: 'What components are required in a test dataset to evaluate AI?'\n",
      "  Query 2/3: 'How to run evaluations in Evidently?...'\n",
      "Hybrid search found 5 results for query: 'How to run evaluations in Evidently?'\n",
      "  Query 3/3: 'Understanding data definition and descriptors...'\n",
      "Hybrid search found 5 results for query: 'Understanding data definition and descriptors'\n",
      "  â€¢ Hit Rate: 66.67%\n",
      "  â€¢ MRR: 0.500\n",
      "  â€¢ Precision@5: 33.33%\n",
      "\n",
      " BEST METHODS:\n",
      "  â€¢ Best Hit Rate: VECTOR\n",
      "  â€¢ Best MRR: VECTOR\n",
      "  â€¢ Best Precision: VECTOR\n",
      "\n",
      "ðŸ“ˆ Hybrid vs Text Search Improvement:\n",
      "  â€¢ Hit Rate: +33.3%\n",
      "  â€¢ MRR: +0.167\n"
     ]
    }
   ],
   "source": [
    "def compare_search_methods(test_queries):\n",
    "    \"\"\"\n",
    "    Compare text, vector, and hybrid search performance.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" COMPARING SEARCH METHODS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    methods = {\n",
    "        'text': lambda q: text_search(q),\n",
    "        'vector': lambda q: vector_search(q),\n",
    "        'hybrid': lambda q: hybrid_search(q, alpha=0.5)\n",
    "    }\n",
    "    \n",
    "    comparison_results = {}\n",
    "    \n",
    "    for method_name, search_fn in methods.items():\n",
    "        print(f\"\\nðŸ“Š Evaluating {method_name.upper()} search...\")\n",
    "        \n",
    "        eval_results = evaluate_search_quality(\n",
    "            search_function=search_fn,\n",
    "            test_queries=test_queries,\n",
    "            num_results=5,\n",
    "            log_results=False  # Don't log intermediate results\n",
    "        )\n",
    "        \n",
    "        metrics = eval_results['metrics']\n",
    "        comparison_results[method_name] = {\n",
    "            'hit_rate': metrics['hit_rate'],\n",
    "            'mrr': metrics['mean_reciprocal_rank'],\n",
    "            'precision': metrics['mean_precision_at_k'],\n",
    "            'detailed_results': eval_results['detailed_results']\n",
    "        }\n",
    "        \n",
    "        print(f\"  â€¢ Hit Rate: {metrics['hit_rate']:.2%}\")\n",
    "        print(f\"  â€¢ MRR: {metrics['mean_reciprocal_rank']:.3f}\")\n",
    "        print(f\"  â€¢ Precision@5: {metrics['mean_precision_at_k']:.2%}\")\n",
    "    \n",
    "    # Find best method for each metric\n",
    "    print(\"\\n BEST METHODS:\")\n",
    "    print(f\"  â€¢ Best Hit Rate: {max(comparison_results.items(), key=lambda x: x[1]['hit_rate'])[0].upper()}\")\n",
    "    print(f\"  â€¢ Best MRR: {max(comparison_results.items(), key=lambda x: x[1]['mrr'])[0].upper()}\")\n",
    "    print(f\"  â€¢ Best Precision: {max(comparison_results.items(), key=lambda x: x[1]['precision'])[0].upper()}\")\n",
    "    \n",
    "    # Show improvement from text to hybrid\n",
    "    if 'text' in comparison_results and 'hybrid' in comparison_results:\n",
    "        hit_rate_improvement = comparison_results['hybrid']['hit_rate'] - comparison_results['text']['hit_rate']\n",
    "        mrr_improvement = comparison_results['hybrid']['mrr'] - comparison_results['text']['mrr']\n",
    "        \n",
    "        print(f\"\\nðŸ“ˆ Hybrid vs Text Search Improvement:\")\n",
    "        print(f\"  â€¢ Hit Rate: {hit_rate_improvement:+.1%}\")\n",
    "        print(f\"  â€¢ MRR: {mrr_improvement:+.3f}\")\n",
    "    \n",
    "    return comparison_results\n",
    "\n",
    "# Run comparison\n",
    "comparison = compare_search_methods(test_queries)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62c5deb3-85c3-463f-af57-6495d59bf7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " OPTIMIZING HYBRID SEARCH ALPHA\n",
      "============================================================\n",
      "\n",
      "ðŸ”§ Testing alpha=0.00\n",
      "Starting evaluation with 3 test queries...\n",
      "  Query 1/3: 'What components are required in a test dataset to ...'\n",
      "Hybrid search found 5 results for query: 'What components are required in a test dataset to evaluate AI?'\n",
      "  Query 2/3: 'How to run evaluations in Evidently?...'\n",
      "Hybrid search found 5 results for query: 'How to run evaluations in Evidently?'\n",
      "  Query 3/3: 'Understanding data definition and descriptors...'\n",
      "Hybrid search found 5 results for query: 'Understanding data definition and descriptors'\n",
      "  â€¢ Hit Rate: 100.00%\n",
      "  â€¢ MRR: 0.567\n",
      "\n",
      "ðŸ”§ Testing alpha=0.25\n",
      "Starting evaluation with 3 test queries...\n",
      "  Query 1/3: 'What components are required in a test dataset to ...'\n",
      "Hybrid search found 5 results for query: 'What components are required in a test dataset to evaluate AI?'\n",
      "  Query 2/3: 'How to run evaluations in Evidently?...'\n",
      "Hybrid search found 5 results for query: 'How to run evaluations in Evidently?'\n",
      "  Query 3/3: 'Understanding data definition and descriptors...'\n",
      "Hybrid search found 5 results for query: 'Understanding data definition and descriptors'\n",
      "  â€¢ Hit Rate: 100.00%\n",
      "  â€¢ MRR: 0.567\n",
      "\n",
      "ðŸ”§ Testing alpha=0.50\n",
      "Starting evaluation with 3 test queries...\n",
      "  Query 1/3: 'What components are required in a test dataset to ...'\n",
      "Hybrid search found 5 results for query: 'What components are required in a test dataset to evaluate AI?'\n",
      "  Query 2/3: 'How to run evaluations in Evidently?...'\n",
      "Hybrid search found 5 results for query: 'How to run evaluations in Evidently?'\n",
      "  Query 3/3: 'Understanding data definition and descriptors...'\n",
      "Hybrid search found 5 results for query: 'Understanding data definition and descriptors'\n",
      "  â€¢ Hit Rate: 66.67%\n",
      "  â€¢ MRR: 0.500\n",
      "\n",
      "ðŸ”§ Testing alpha=0.75\n",
      "Starting evaluation with 3 test queries...\n",
      "  Query 1/3: 'What components are required in a test dataset to ...'\n",
      "Hybrid search found 5 results for query: 'What components are required in a test dataset to evaluate AI?'\n",
      "  Query 2/3: 'How to run evaluations in Evidently?...'\n",
      "Hybrid search found 5 results for query: 'How to run evaluations in Evidently?'\n",
      "  Query 3/3: 'Understanding data definition and descriptors...'\n",
      "Hybrid search found 5 results for query: 'Understanding data definition and descriptors'\n",
      "  â€¢ Hit Rate: 33.33%\n",
      "  â€¢ MRR: 0.333\n",
      "\n",
      "ðŸ”§ Testing alpha=1.00\n",
      "Starting evaluation with 3 test queries...\n",
      "  Query 1/3: 'What components are required in a test dataset to ...'\n",
      "Hybrid search found 5 results for query: 'What components are required in a test dataset to evaluate AI?'\n",
      "  Query 2/3: 'How to run evaluations in Evidently?...'\n",
      "Hybrid search found 5 results for query: 'How to run evaluations in Evidently?'\n",
      "  Query 3/3: 'Understanding data definition and descriptors...'\n",
      "Hybrid search found 5 results for query: 'Understanding data definition and descriptors'\n",
      "  â€¢ Hit Rate: 33.33%\n",
      "  â€¢ MRR: 0.333\n",
      "\n",
      "ðŸŽ¯ OPTIMAL ALPHA VALUES:\n",
      "  â€¢ Best for MRR: Î±=0.00 (MRR=0.567)\n",
      "  â€¢ Best for Hit Rate: Î±=0.00 (Hit Rate=100.00%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def optimize_hybrid_alpha(test_queries, alphas=[0.0, 0.25, 0.5, 0.75, 1.0]):\n",
    "    \"\"\"\n",
    "    Find the optimal alpha value for hybrid search.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" OPTIMIZING HYBRID SEARCH ALPHA\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        print(f\"\\nðŸ”§ Testing alpha={alpha:.2f}\")\n",
    "        \n",
    "        hybrid_fn = lambda q: hybrid_search(q, alpha=alpha)\n",
    "        \n",
    "        eval_results = evaluate_search_quality(\n",
    "            search_function=hybrid_fn,\n",
    "            test_queries=test_queries,\n",
    "            num_results=5,\n",
    "            log_results=False\n",
    "        )\n",
    "        \n",
    "        metrics = eval_results['metrics']\n",
    "        results.append({\n",
    "            'alpha': alpha,\n",
    "            'hit_rate': metrics['hit_rate'],\n",
    "            'mrr': metrics['mean_reciprocal_rank'],\n",
    "            'precision': metrics['mean_precision_at_k']\n",
    "        })\n",
    "        \n",
    "        print(f\"  â€¢ Hit Rate: {metrics['hit_rate']:.2%}\")\n",
    "        print(f\"  â€¢ MRR: {metrics['mean_reciprocal_rank']:.3f}\")\n",
    "    \n",
    "    # Find optimal alpha\n",
    "    best_by_mrr = max(results, key=lambda x: x['mrr'])\n",
    "    best_by_hit_rate = max(results, key=lambda x: x['hit_rate'])\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ OPTIMAL ALPHA VALUES:\")\n",
    "    print(f\"  â€¢ Best for MRR: Î±={best_by_mrr['alpha']:.2f} (MRR={best_by_mrr['mrr']:.3f})\")\n",
    "    print(f\"  â€¢ Best for Hit Rate: Î±={best_by_hit_rate['alpha']:.2f} (Hit Rate={best_by_hit_rate['hit_rate']:.2%})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Optimize alpha\n",
    "alpha_results = optimize_hybrid_alpha(test_queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f5e423a-90e8-4805-bc4a-f65a8e23bfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸš€ FINAL EVALUATION WITH OPTIMIZED HYBRID SEARCH\n",
      "============================================================\n",
      "Starting evaluation with 3 test queries...\n",
      "  Query 1/3: 'What components are required in a test dataset to ...'\n",
      "Hybrid search found 5 results for query: 'What components are required in a test dataset to evaluate AI?'\n",
      "  Query 2/3: 'How to run evaluations in Evidently?...'\n",
      "Hybrid search found 5 results for query: 'How to run evaluations in Evidently?'\n",
      "  Query 3/3: 'Understanding data definition and descriptors...'\n",
      "Hybrid search found 5 results for query: 'Understanding data definition and descriptors'\n",
      "ðŸ“ Evaluation results saved to: logs/search_evaluation_20251003_051943_a2ff82.json\n",
      "\n",
      "============================================================\n",
      "ðŸ“Š EVALUATION REPORT\n",
      "============================================================\n",
      "Timestamp: 2025-10-03T05:19:43.380922\n",
      "Number of queries: 3\n",
      "Results per query: 5\n",
      "\n",
      "ðŸ“ˆ AGGREGATE METRICS:\n",
      "  â€¢ Hit Rate: 66.67%\n",
      "  â€¢ Mean Reciprocal Rank (MRR): 0.500\n",
      "  â€¢ Mean Precision@5: 33.33%\n",
      "\n",
      "ðŸ” QUERY-LEVEL RESULTS:\n",
      "\n",
      "  Query 1: \"What components are required in a test dataset to ...\"\n",
      "    â€¢ Hit: âœ…\n",
      "    â€¢ MRR: 0.500\n",
      "    â€¢ Precision: 40.00%\n",
      "    â€¢ First relevant at rank: 2\n",
      "\n",
      "  Query 2: \"How to run evaluations in Evidently?...\"\n",
      "    â€¢ Hit: âŒ\n",
      "    â€¢ MRR: 0.000\n",
      "    â€¢ Precision: 0.00%\n",
      "\n",
      "  Query 3: \"Understanding data definition and descriptors...\"\n",
      "    â€¢ Hit: âœ…\n",
      "    â€¢ MRR: 1.000\n",
      "    â€¢ Precision: 60.00%\n",
      "    â€¢ First relevant at rank: 1\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸš€ FINAL EVALUATION WITH OPTIMIZED HYBRID SEARCH\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use the best alpha from optimization (you can adjust based on results)\n",
    "best_alpha = 0.5  # Adjust based on optimization results\n",
    "\n",
    "final_search = lambda q: hybrid_search(q, alpha=best_alpha)\n",
    "\n",
    "final_evaluation = evaluate_search_quality(\n",
    "    search_function=final_search,\n",
    "    test_queries=test_queries,\n",
    "    num_results=5,\n",
    "    log_results=True\n",
    ")\n",
    "\n",
    "print_evaluation_report(final_evaluation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f7ddf10-5dfc-44af-9d06-4795b7edce2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LLM-based evaluation with rate limit management...\n",
      "Rate limit: 16 requests per minute\n",
      "\n",
      "Evaluating case 1/1\n",
      "  Searching for: 'What components are required in a test dataset to ...'\n",
      "Hybrid search found 5 results for query: 'What components are required in a test dataset to evaluate AI?'\n",
      "  Generating answer...\n",
      "Hybrid search found 5 results for query: 'What components are required in a test dataset to evaluate AI?'\n",
      "  Evaluating overall_quality...\n",
      "  Evaluating relevance...\n",
      "  â³ Rate limit hit. Waiting 2.0s before retry 1/3...\n",
      "\n",
      "============================================================\n",
      "ðŸ“Š LLM EVALUATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "Total test cases evaluated: 1\n",
      "Average overall score: 3.00/5.0\n",
      "Score standard deviation: nan\n",
      "\n",
      "ðŸ“ˆ SCORES BY CRITERIA:\n",
      "  â€¢ Overall Quality: 3.00\n",
      "  â€¢ Relevance: 3.00\n",
      "\n",
      "ðŸ† BEST PERFORMING QUERY:\n",
      "  Query: What components are required in a test dataset to evaluate AI?\n",
      "  Score: 3.00\n",
      "\n",
      "âš ï¸ NEEDS IMPROVEMENT:\n",
      "  Query: What components are required in a test dataset to evaluate AI?\n",
      "  Score: 3.00\n",
      "\n",
      "ðŸ“Š SCORE DISTRIBUTION:\n",
      "  â€¢ Excellent (4.5-5.0): 0 (0.0%)\n",
      "  â€¢ Good (3.5-4.5): 0 (0.0%)\n",
      "  â€¢ Fair (2.5-3.5): 1 (100.0%)\n",
      "  â€¢ Poor (1.0-2.5): 0 (0.0%)\n",
      "\n",
      "============================================================\n",
      "ðŸ“ Results saved to: logs/llm_eval_20251003_052104_c06a02.json\n",
      "\n",
      "============================================================\n",
      "ðŸ” COMPARING SEARCH METHODS WITH LLM EVALUATION\n",
      "============================================================\n",
      "\n",
      "Query: 'What components are required in a test dataset to ...'\n",
      "  Testing text search...\n",
      "Text search found 5 results for query: 'What components are required in a test dataset to evaluate AI?'\n",
      "  â³ Rate limit hit. Waiting 2.0s before retry 1/3...\n",
      "  Testing vector search...\n",
      "Vector search found 5 results for query: 'What components are required in a test dataset to evaluate AI?'\n",
      "  Testing hybrid search...\n",
      "Hybrid search found 5 results for query: 'What components are required in a test dataset to evaluate AI?'\n",
      "\n",
      "Query: 'How to run evaluations in Evidently?...'\n",
      "  Testing text search...\n",
      "Text search found 5 results for query: 'How to run evaluations in Evidently?'\n",
      "  â³ Rate limit hit. Waiting 2.0s before retry 1/3...\n",
      "  Testing vector search...\n",
      "Vector search found 5 results for query: 'How to run evaluations in Evidently?'\n",
      "  Testing hybrid search...\n",
      "Hybrid search found 5 results for query: 'How to run evaluations in Evidently?'\n",
      "\n",
      "Query: 'Understanding data definition and descriptors...'\n",
      "  Testing text search...\n",
      "Text search found 5 results for query: 'Understanding data definition and descriptors'\n",
      "  Testing vector search...\n",
      "Vector search found 5 results for query: 'Understanding data definition and descriptors'\n",
      "  Testing hybrid search...\n",
      "Hybrid search found 5 results for query: 'Understanding data definition and descriptors'\n",
      "  â³ Rate limit hit. Waiting 60.0s before retry 1/3...\n",
      "  â³ Rate limit hit. Waiting 60.0s before retry 2/3...\n",
      "  â³ Rate limit hit. Waiting 60.0s before retry 3/3...\n",
      "  âŒ Rate limit exceeded after 3 retries\n",
      "\n",
      "ðŸ“Š METHOD COMPARISON RESULTS:\n",
      "  â€¢ TEXT: 3.67\n",
      "  â€¢ VECTOR: 2.67\n",
      "  â€¢ HYBRID: 2.33\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "@dataclass\n",
    "class LLMEvaluationResult:\n",
    "    \"\"\"Store LLM-based evaluation results\"\"\"\n",
    "    query: str\n",
    "    response: str\n",
    "    criteria: str\n",
    "    score: float\n",
    "    reasoning: str\n",
    "    timestamp: str = None\n",
    "    metadata: Dict[str, Any] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.timestamp is None:\n",
    "            self.timestamp = datetime.now().isoformat()\n",
    "\n",
    "class LLMEvaluator:\n",
    "    \"\"\"\n",
    "    LLM-based evaluator for agent responses using OpenRouter/DeepSeek\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, client=None, model=\"deepseek/deepseek-r1:free\"):\n",
    "        \"\"\"\n",
    "        Initialize with existing OpenRouter client\n",
    "        \n",
    "        Args:\n",
    "            client: OpenAI client configured for OpenRouter\n",
    "            model: Model to use for evaluation\n",
    "        \"\"\"\n",
    "        self.client = client if client else openai_client  # Use existing client\n",
    "        self.model = model\n",
    "        self.evaluation_history = []\n",
    "    \n",
    "    def evaluate_response(self,\n",
    "                         query: str,\n",
    "                         response: str,\n",
    "                         criteria: str = \"overall_quality\",\n",
    "                         reference: Optional[str] = None,\n",
    "                         context: Optional[str] = None,\n",
    "                         max_retries: int = 3) -> LLMEvaluationResult:\n",
    "        \"\"\"\n",
    "        Evaluate a single agent response using LLM with retry logic\n",
    "        \n",
    "        Args:\n",
    "            query: The original user query\n",
    "            response: The agent's response\n",
    "            criteria: Evaluation criteria to use\n",
    "            reference: Optional reference answer for comparison\n",
    "            context: Optional context/documents used by agent\n",
    "            max_retries: Maximum number of retry attempts for rate limits\n",
    "        \"\"\"\n",
    "        \n",
    "        prompt = self._build_evaluation_prompt(\n",
    "            query, response, criteria, reference, context\n",
    "        )\n",
    "        \n",
    "        import time\n",
    "        \n",
    "        for attempt in range(max_retries + 1):\n",
    "            try:\n",
    "                llm_response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are an expert evaluator for AI agent responses. Provide scores from 1-5 with detailed reasoning.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    temperature=0.0,  # Deterministic for consistency\n",
    "                    max_tokens=500\n",
    "                )\n",
    "                \n",
    "                # Parse the response\n",
    "                result_text = llm_response.choices[0].message.content\n",
    "                parsed = self._parse_evaluation(result_text)\n",
    "                \n",
    "                eval_result = LLMEvaluationResult(\n",
    "                    query=query,\n",
    "                    response=response,\n",
    "                    criteria=criteria,\n",
    "                    score=parsed['score'],\n",
    "                    reasoning=parsed['reasoning'],\n",
    "                    metadata={'model_used': self.model, 'attempts': attempt + 1}\n",
    "                )\n",
    "                \n",
    "                self.evaluation_history.append(eval_result)\n",
    "                return eval_result\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_str = str(e)\n",
    "                \n",
    "                # Check if it's a rate limit error\n",
    "                if '429' in error_str or 'rate limit' in error_str.lower():\n",
    "                    if attempt < max_retries:\n",
    "                        # Extract wait time if provided, otherwise use exponential backoff\n",
    "                        wait_time = 2 ** (attempt + 1)  # 2, 4, 8 seconds\n",
    "                        \n",
    "                        # Try to extract reset time from error if available\n",
    "                        if 'X-RateLimit-Reset' in error_str:\n",
    "                            try:\n",
    "                                import re\n",
    "                                reset_match = re.search(r'X-RateLimit-Reset.*?(\\d+)', error_str)\n",
    "                                if reset_match:\n",
    "                                    reset_time = int(reset_match.group(1)) / 1000  # Convert from ms\n",
    "                                    current_time = time.time()\n",
    "                                    wait_time = max(1, min(reset_time - current_time, 60))\n",
    "                            except:\n",
    "                                pass\n",
    "                        \n",
    "                        print(f\"  â³ Rate limit hit. Waiting {wait_time:.1f}s before retry {attempt + 1}/{max_retries}...\")\n",
    "                        time.sleep(wait_time)\n",
    "                        continue\n",
    "                    else:\n",
    "                        print(f\"  âŒ Rate limit exceeded after {max_retries} retries\")\n",
    "                        return LLMEvaluationResult(\n",
    "                            query=query,\n",
    "                            response=response,\n",
    "                            criteria=criteria,\n",
    "                            score=3,  # Default neutral score\n",
    "                            reasoning=f\"Could not evaluate due to rate limits after {max_retries} retries\"\n",
    "                        )\n",
    "                else:\n",
    "                    # Other errors - don't retry\n",
    "                    print(f\"Evaluation error: {e}\")\n",
    "                    return LLMEvaluationResult(\n",
    "                        query=query,\n",
    "                        response=response,\n",
    "                        criteria=criteria,\n",
    "                        score=0,\n",
    "                        reasoning=f\"Error during evaluation: {str(e)}\"\n",
    "                    )\n",
    "    \n",
    "    def _build_evaluation_prompt(self, \n",
    "                                query: str, \n",
    "                                response: str,\n",
    "                                criteria: str,\n",
    "                                reference: Optional[str] = None,\n",
    "                                context: Optional[str] = None) -> str:\n",
    "        \"\"\"Build evaluation prompt based on criteria\"\"\"\n",
    "        \n",
    "        base_prompt = f\"\"\"Evaluate the following AI agent response.\n",
    "\n",
    "User Query: {query}\n",
    "\n",
    "Agent Response: {response}\n",
    "\n",
    "{f\"Reference Answer: {reference}\" if reference else \"\"}\n",
    "{f\"Context Used: {context[:500]}...\" if context else \"\"}\n",
    "\n",
    "Evaluation Criteria: {criteria}\n",
    "\"\"\"\n",
    "        \n",
    "        criteria_instructions = {\n",
    "            \"overall_quality\": \"\"\"\n",
    "Rate the overall quality considering:\n",
    "1. Relevance - Does it address the query?\n",
    "2. Accuracy - Is the information correct?\n",
    "3. Completeness - Does it fully answer the question?\n",
    "4. Clarity - Is it well-structured and clear?\n",
    "5. Usefulness - Would this help the user?\n",
    "\n",
    "Provide a score from 1-5 and explain your reasoning.\n",
    "Format your response as:\n",
    "SCORE: [1-5]\n",
    "REASONING: [Your detailed explanation]\"\"\",\n",
    "            \n",
    "            \"relevance\": \"\"\"\n",
    "Rate how relevant the response is to the query (1-5).\n",
    "1 = Completely off-topic\n",
    "2 = Mostly irrelevant\n",
    "3 = Partially relevant\n",
    "4 = Mostly relevant\n",
    "5 = Perfectly relevant\n",
    "\n",
    "Format: SCORE: [1-5] REASONING: [explanation]\"\"\",\n",
    "            \n",
    "            \"accuracy\": \"\"\"\n",
    "Rate the factual accuracy (1-5).\n",
    "1 = Contains major errors\n",
    "2 = Several inaccuracies\n",
    "3 = Mostly accurate\n",
    "4 = Very accurate\n",
    "5 = Completely accurate\n",
    "\n",
    "Format: SCORE: [1-5] REASONING: [explanation]\"\"\",\n",
    "            \n",
    "            \"coherence\": \"\"\"\n",
    "Rate the coherence and clarity (1-5).\n",
    "1 = Incoherent/confusing\n",
    "2 = Poor structure\n",
    "3 = Acceptable clarity\n",
    "4 = Well-structured\n",
    "5 = Excellent clarity\n",
    "\n",
    "Format: SCORE: [1-5] REASONING: [explanation]\"\"\",\n",
    "            \n",
    "            \"faithfulness\": \"\"\"\n",
    "Rate how faithful the response is to the provided context (1-5).\n",
    "1 = Contradicts context\n",
    "2 = Mostly unsupported\n",
    "3 = Partially supported\n",
    "4 = Well-supported\n",
    "5 = Fully grounded in context\n",
    "\n",
    "Format: SCORE: [1-5] REASONING: [explanation]\"\"\"\n",
    "        }\n",
    "        \n",
    "        return base_prompt + criteria_instructions.get(\n",
    "            criteria, \n",
    "            criteria_instructions[\"overall_quality\"]\n",
    "        )\n",
    "    \n",
    "    def _parse_evaluation(self, response_text: str) -> Dict:\n",
    "        \"\"\"Parse LLM evaluation response\"\"\"\n",
    "        \n",
    "        try:\n",
    "            lines = response_text.strip().split('\\n')\n",
    "            score = 3  # Default\n",
    "            reasoning = \"\"\n",
    "            \n",
    "            for i, line in enumerate(lines):\n",
    "                if 'SCORE:' in line.upper():\n",
    "                    # Extract numeric score\n",
    "                    import re\n",
    "                    numbers = re.findall(r'\\d', line)\n",
    "                    if numbers:\n",
    "                        score = int(numbers[0])\n",
    "                        score = min(max(score, 1), 5)\n",
    "                elif 'REASONING:' in line.upper():\n",
    "                    # Get all text after REASONING:\n",
    "                    reasoning_start = line.upper().index('REASONING:') + 10\n",
    "                    reasoning = line[reasoning_start:].strip()\n",
    "                    # Add any subsequent lines\n",
    "                    if i < len(lines) - 1:\n",
    "                        reasoning += ' ' + ' '.join(lines[i+1:])\n",
    "                    break\n",
    "            \n",
    "            return {\n",
    "                'score': score,\n",
    "                'reasoning': reasoning if reasoning else response_text\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'score': 3,\n",
    "                'reasoning': response_text[:200]\n",
    "            }\n",
    "\n",
    "# %%\n",
    "class AgentResponseEvaluator:\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation suite for agent responses\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, search_function, answer_function, llm_evaluator=None):\n",
    "        \"\"\"\n",
    "        Initialize with search and answer functions\n",
    "        \n",
    "        Args:\n",
    "            search_function: Function to search for context\n",
    "            answer_function: Function to generate answers\n",
    "            llm_evaluator: LLMEvaluator instance\n",
    "        \"\"\"\n",
    "        self.search_function = search_function\n",
    "        self.answer_function = answer_function\n",
    "        self.llm_evaluator = llm_evaluator or LLMEvaluator()\n",
    "        self.results = []\n",
    "    \n",
    "    def evaluate_end_to_end(self,\n",
    "                           test_cases: List[Dict[str, str]],\n",
    "                           criteria: List[str] = None,\n",
    "                           batch_delay: float = 1.0) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Evaluate complete RAG pipeline with rate limiting management\n",
    "        \n",
    "        Args:\n",
    "            test_cases: List of dicts with 'query' and optionally 'reference'\n",
    "            criteria: Evaluation criteria to use\n",
    "            batch_delay: Delay between evaluations to avoid rate limits\n",
    "        \"\"\"\n",
    "        \n",
    "        if criteria is None:\n",
    "            criteria = [\"overall_quality\", \"relevance\", \"accuracy\", \"faithfulness\"]\n",
    "        \n",
    "        results = []\n",
    "        import time\n",
    "        \n",
    "        for i, test_case in enumerate(test_cases):\n",
    "            print(f\"\\nEvaluating case {i+1}/{len(test_cases)}\")\n",
    "            query = test_case['query']\n",
    "            reference = test_case.get('reference', None)\n",
    "            \n",
    "            # Get search results\n",
    "            print(f\"  Searching for: '{query[:50]}...'\")\n",
    "            search_results = self.search_function(query)\n",
    "            \n",
    "            # Generate answer\n",
    "            print(\"  Generating answer...\")\n",
    "            answer = self.answer_function(query, 'hybrid')\n",
    "            \n",
    "            # Format context from search results\n",
    "            context = \"\\n\".join([\n",
    "                f\"{r.get('chunk', '')[:200]}...\" \n",
    "                for r in search_results[:3]\n",
    "            ])\n",
    "            \n",
    "            # Evaluate with each criteria\n",
    "            case_result = {\n",
    "                'case_id': i,\n",
    "                'query': query[:100],\n",
    "                'response': answer[:200],\n",
    "                'has_reference': reference is not None\n",
    "            }\n",
    "            \n",
    "            for j, criterion in enumerate(criteria):\n",
    "                print(f\"  Evaluating {criterion}...\")\n",
    "                \n",
    "                # Add delay between API calls to avoid rate limits\n",
    "                if j > 0:\n",
    "                    time.sleep(batch_delay)\n",
    "                \n",
    "                eval_result = self.llm_evaluator.evaluate_response(\n",
    "                    query=query,\n",
    "                    response=answer,\n",
    "                    criteria=criterion,\n",
    "                    reference=reference,\n",
    "                    context=context,\n",
    "                    max_retries=3  # Use retry logic\n",
    "                )\n",
    "                \n",
    "                case_result[f'{criterion}_score'] = eval_result.score\n",
    "                case_result[f'{criterion}_reasoning'] = eval_result.reasoning[:100]\n",
    "            \n",
    "            # Calculate average score\n",
    "            score_cols = [k for k in case_result.keys() if k.endswith('_score')]\n",
    "            case_result['avg_score'] = np.mean([case_result[col] for col in score_cols])\n",
    "            \n",
    "            results.append(case_result)\n",
    "            self.results = results\n",
    "            \n",
    "            # Add delay between test cases\n",
    "            if i < len(test_cases) - 1:\n",
    "                print(f\"  â±ï¸ Waiting {batch_delay}s before next case...\")\n",
    "                time.sleep(batch_delay)\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def print_evaluation_summary(self, df: pd.DataFrame):\n",
    "        \"\"\"Print a formatted summary of evaluation results\"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ðŸ“Š LLM EVALUATION SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Overall statistics\n",
    "        print(f\"\\nTotal test cases evaluated: {len(df)}\")\n",
    "        print(f\"Average overall score: {df['avg_score'].mean():.2f}/5.0\")\n",
    "        print(f\"Score standard deviation: {df['avg_score'].std():.2f}\")\n",
    "        \n",
    "        # Score breakdown by criteria\n",
    "        score_cols = [col for col in df.columns if col.endswith('_score') and col != 'avg_score']\n",
    "        \n",
    "        print(\"\\nðŸ“ˆ SCORES BY CRITERIA:\")\n",
    "        for col in score_cols:\n",
    "            criteria_name = col.replace('_score', '').replace('_', ' ').title()\n",
    "            mean_score = df[col].mean()\n",
    "            print(f\"  â€¢ {criteria_name}: {mean_score:.2f}\")\n",
    "        \n",
    "        # Best and worst performing cases\n",
    "        if len(df) > 0:\n",
    "            best_idx = df['avg_score'].idxmax()\n",
    "            worst_idx = df['avg_score'].idxmin()\n",
    "            \n",
    "            print(\"\\nðŸ† BEST PERFORMING QUERY:\")\n",
    "            print(f\"  Query: {df.loc[best_idx, 'query']}\")\n",
    "            print(f\"  Score: {df.loc[best_idx, 'avg_score']:.2f}\")\n",
    "            \n",
    "            print(\"\\nâš ï¸ NEEDS IMPROVEMENT:\")\n",
    "            print(f\"  Query: {df.loc[worst_idx, 'query']}\")\n",
    "            print(f\"  Score: {df.loc[worst_idx, 'avg_score']:.2f}\")\n",
    "        \n",
    "        # Distribution of scores\n",
    "        print(\"\\nðŸ“Š SCORE DISTRIBUTION:\")\n",
    "        score_ranges = {\n",
    "            'Excellent (4.5-5.0)': ((df['avg_score'] >= 4.5) & (df['avg_score'] <= 5.0)).sum(),\n",
    "            'Good (3.5-4.5)': ((df['avg_score'] >= 3.5) & (df['avg_score'] < 4.5)).sum(),\n",
    "            'Fair (2.5-3.5)': ((df['avg_score'] >= 2.5) & (df['avg_score'] < 3.5)).sum(),\n",
    "            'Poor (1.0-2.5)': ((df['avg_score'] >= 1.0) & (df['avg_score'] < 2.5)).sum()\n",
    "        }\n",
    "        \n",
    "        for range_name, count in score_ranges.items():\n",
    "            percentage = (count / len(df) * 100) if len(df) > 0 else 0\n",
    "            print(f\"  â€¢ {range_name}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    def save_evaluation_results(self, df: pd.DataFrame, filename_prefix: str = \"llm_eval\") -> Path:\n",
    "        \"\"\"Save LLM evaluation results to file\"\"\"\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        rand_hex = secrets.token_hex(3)\n",
    "        filename = f\"{filename_prefix}_{timestamp}_{rand_hex}.json\"\n",
    "        filepath = LOG_DIR / filename\n",
    "        \n",
    "        # Convert DataFrame to dict for JSON serialization\n",
    "        results_dict = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'summary': {\n",
    "                'total_cases': len(df),\n",
    "                'avg_score': df['avg_score'].mean() if 'avg_score' in df.columns else 0,\n",
    "                'criteria_scores': {}\n",
    "            },\n",
    "            'detailed_results': df.to_dict('records')\n",
    "        }\n",
    "        \n",
    "        # Add criteria scores to summary\n",
    "        score_cols = [col for col in df.columns if col.endswith('_score') and col != 'avg_score']\n",
    "        for col in score_cols:\n",
    "            criteria_name = col.replace('_score', '')\n",
    "            results_dict['summary']['criteria_scores'][criteria_name] = df[col].mean()\n",
    "        \n",
    "        with filepath.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results_dict, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"ðŸ“ Results saved to: {filepath}\")\n",
    "        return filepath\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Rate Limit Management for Free API Tier\n",
    "\n",
    "# %%\n",
    "class RateLimitManager:\n",
    "    \"\"\"\n",
    "    Manage rate limits for free API tier\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, requests_per_minute: int = 16):\n",
    "        \"\"\"\n",
    "        Initialize rate limit manager\n",
    "        \n",
    "        Args:\n",
    "            requests_per_minute: API limit for free tier (default 16 for DeepSeek free)\n",
    "        \"\"\"\n",
    "        self.requests_per_minute = requests_per_minute\n",
    "        self.request_times = []\n",
    "        import time\n",
    "        self.time = time\n",
    "    \n",
    "    def wait_if_needed(self):\n",
    "        \"\"\"\n",
    "        Wait if necessary to avoid rate limits\n",
    "        \"\"\"\n",
    "        current_time = self.time.time()\n",
    "        \n",
    "        # Remove requests older than 1 minute\n",
    "        self.request_times = [t for t in self.request_times if current_time - t < 60]\n",
    "        \n",
    "        # If we've hit the limit, wait\n",
    "        if len(self.request_times) >= self.requests_per_minute:\n",
    "            oldest_request = min(self.request_times)\n",
    "            wait_time = 61 - (current_time - oldest_request)\n",
    "            if wait_time > 0:\n",
    "                print(f\"  â³ Rate limit approaching. Waiting {wait_time:.1f}s...\")\n",
    "                self.time.sleep(wait_time)\n",
    "                # Clean up old requests after waiting\n",
    "                current_time = self.time.time()\n",
    "                self.request_times = [t for t in self.request_times if current_time - t < 60]\n",
    "    \n",
    "    def record_request(self):\n",
    "        \"\"\"\n",
    "        Record a request time\n",
    "        \"\"\"\n",
    "        self.request_times.append(self.time.time())\n",
    "\n",
    "# Initialize global rate limiter for the notebook\n",
    "rate_limiter = RateLimitManager(requests_per_minute=16)\n",
    "\n",
    "# Modified LLM evaluator with rate limiting\n",
    "class LLMEvaluatorWithRateLimit(LLMEvaluator):\n",
    "    \"\"\"\n",
    "    LLM Evaluator with built-in rate limiting for free tier\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, client=None, model=\"deepseek/deepseek-r1:free\", rate_limiter=None):\n",
    "        super().__init__(client, model)\n",
    "        self.rate_limiter = rate_limiter or RateLimitManager()\n",
    "    \n",
    "    def evaluate_response(self,\n",
    "                         query: str,\n",
    "                         response: str,\n",
    "                         criteria: str = \"overall_quality\",\n",
    "                         reference: Optional[str] = None,\n",
    "                         context: Optional[str] = None,\n",
    "                         max_retries: int = 3) -> LLMEvaluationResult:\n",
    "        \"\"\"\n",
    "        Evaluate with automatic rate limiting\n",
    "        \"\"\"\n",
    "        # Wait if necessary before making request\n",
    "        self.rate_limiter.wait_if_needed()\n",
    "        \n",
    "        # Call parent method with retry logic\n",
    "        result = super().evaluate_response(\n",
    "            query, response, criteria, reference, context, max_retries\n",
    "        )\n",
    "        \n",
    "        # Record successful request\n",
    "        if result.score > 0:  # Only count successful requests\n",
    "            self.rate_limiter.record_request()\n",
    "        \n",
    "        return result\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Example Usage: Evaluating Your Agent Responses with LLM\n",
    "\n",
    "# %%\n",
    "# Initialize the LLM evaluator with rate limiting\n",
    "llm_evaluator = LLMEvaluatorWithRateLimit(\n",
    "    client=openai_client,\n",
    "    rate_limiter=rate_limiter\n",
    ")\n",
    "\n",
    "# Test cases for LLM evaluation - start small to test rate limiting\n",
    "llm_test_cases = [\n",
    "    {\n",
    "        'query': \"What components are required in a test dataset to evaluate AI?\",\n",
    "        'reference': \"A test dataset should include questions, expected answers, and optionally context.\"\n",
    "    },\n",
    "    {\n",
    "        'query': \"How to detect data drift in Evidently?\",\n",
    "        'reference': None  # No reference answer\n",
    "    }\n",
    "]\n",
    "\n",
    "# Initialize the end-to-end evaluator with rate limiting\n",
    "agent_evaluator = AgentResponseEvaluator(\n",
    "    search_function=hybrid_search,\n",
    "    answer_function=answer_question_manual,\n",
    "    llm_evaluator=llm_evaluator\n",
    ")\n",
    "\n",
    "# Run evaluation with automatic rate limit handling\n",
    "print(\"Starting LLM-based evaluation with rate limit management...\")\n",
    "print(f\"Rate limit: {rate_limiter.requests_per_minute} requests per minute\")\n",
    "\n",
    "evaluation_df = agent_evaluator.evaluate_end_to_end(\n",
    "    test_cases=llm_test_cases[:1],  # Start with just 2 cases to test\n",
    "    criteria=[\"overall_quality\", \"relevance\"],  # Fewer criteria to avoid hitting limits\n",
    "    batch_delay=4.0  # 4 second delay between requests (safe for 16 req/min limit)\n",
    ")\n",
    "\n",
    "# Display results\n",
    "agent_evaluator.print_evaluation_summary(evaluation_df)\n",
    "\n",
    "# Save results\n",
    "filepath = agent_evaluator.save_evaluation_results(evaluation_df)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Comparing Search Methods with LLM Evaluation\n",
    "\n",
    "# %%\n",
    "def compare_methods_with_llm(test_queries_subset):\n",
    "    \"\"\"\n",
    "    Compare different search methods using LLM evaluation\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ” COMPARING SEARCH METHODS WITH LLM EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    methods = {\n",
    "        'text': lambda q: answer_question_manual(q, 'text'),\n",
    "        'vector': lambda q: answer_question_manual(q, 'vector'),\n",
    "        'hybrid': lambda q: answer_question_manual(q, 'hybrid')\n",
    "    }\n",
    "    \n",
    "    llm_eval = LLMEvaluator()\n",
    "    comparison_results = []\n",
    "    \n",
    "    for query_text in test_queries_subset:\n",
    "        print(f\"\\nQuery: '{query_text[:50]}...'\")\n",
    "        \n",
    "        for method_name, answer_fn in methods.items():\n",
    "            print(f\"  Testing {method_name} search...\")\n",
    "            \n",
    "            # Generate answer\n",
    "            answer = answer_fn(query_text)\n",
    "            \n",
    "            # Evaluate with LLM\n",
    "            eval_result = llm_eval.evaluate_response(\n",
    "                query=query_text,\n",
    "                response=answer,\n",
    "                criteria=\"overall_quality\"\n",
    "            )\n",
    "            \n",
    "            comparison_results.append({\n",
    "                'query': query_text[:50],\n",
    "                'method': method_name,\n",
    "                'score': eval_result.score,\n",
    "                'reasoning': eval_result.reasoning[:100]\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame and show summary\n",
    "    comparison_df = pd.DataFrame(comparison_results)\n",
    "    \n",
    "    print(\"\\nðŸ“Š METHOD COMPARISON RESULTS:\")\n",
    "    method_avg = comparison_df.groupby('method')['score'].mean().sort_values(ascending=False)\n",
    "    for method, avg_score in method_avg.items():\n",
    "        print(f\"  â€¢ {method.upper()}: {avg_score:.2f}\")\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Example: Compare methods on a subset of queries\n",
    "test_subset = [\n",
    "    \"What components are required in a test dataset to evaluate AI?\",\n",
    "    \"How to run evaluations in Evidently?\",\n",
    "    \"Understanding data definition and descriptors\"\n",
    "]\n",
    "\n",
    "comparison_df = compare_methods_with_llm(test_subset)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Batch LLM Evaluation with Logging\n",
    "\n",
    "# %%\n",
    "def run_comprehensive_llm_evaluation():\n",
    "    \"\"\"\n",
    "    Run comprehensive LLM evaluation on all test queries\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸš€ COMPREHENSIVE LLM EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Convert existing test_queries to LLM test format\n",
    "    llm_test_cases = [\n",
    "        {'query': q[0], 'reference': None} \n",
    "        for q in test_queries\n",
    "    ]\n",
    "    \n",
    "    # Run evaluation\n",
    "    evaluator = AgentResponseEvaluator(\n",
    "        search_function=hybrid_search,\n",
    "        answer_function=answer_question_manual,\n",
    "        llm_evaluator=LLMEvaluator()\n",
    "    )\n",
    "    \n",
    "    results_df = evaluator.evaluate_end_to_end(\n",
    "        test_cases=llm_test_cases,\n",
    "        criteria=[\"relevance\", \"accuracy\", \"coherence\"]\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    evaluator.print_evaluation_summary(results_df)\n",
    "    \n",
    "    # Save to file\n",
    "    filepath = evaluator.save_evaluation_results(results_df, \"comprehensive_llm_eval\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Run comprehensive evaluation\n",
    "# comprehensive_results = run_comprehensive_llm_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae21ed0c-9c9d-46fb-a830-86ea3bb1a37f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3570063-bfbc-4100-b6f3-39ed147f333c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5f1987-91fd-4cfb-8aca-34ea75e33e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06c3534-abea-4077-98d1-e271e144e169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4912c93-7882-4054-b53a-d4a8d0999828",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv)",
   "language": "python",
   "name": "uv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
