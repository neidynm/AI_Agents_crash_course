{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b50e3e2-fef7-48d5-ab02-d34081fe15ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "import logging\n",
    "import asyncio\n",
    "from tqdm import tqdm\n",
    "from minsearch import Index\n",
    "from typing import List, Any\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e4900d3-1f53-4bde-910c-591345b732b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_repo_data(repo_owner, repo_name, branch=\"main\"):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    Yields one document (dict) at a time to avoid loading everything into memory.\n",
    "\n",
    "    Args:\n",
    "        repo_owner (str): GitHub username or organization\n",
    "        repo_name (str): Repository name\n",
    "        branch (str): Branch name (default: main)\n",
    "    \"\"\"\n",
    "    url = f\"https://codeload.github.com/{repo_owner}/{repo_name}/zip/refs/heads/{branch}\"\n",
    "    resp = requests.get(url)\n",
    "\n",
    "    if resp.status_code == 404 and branch == \"main\":\n",
    "        # Try fallback to master\n",
    "        return read_repo_data(repo_owner, repo_name, branch=\"master\")\n",
    "\n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: HTTP {resp.status_code}\")\n",
    "    \n",
    "    with zipfile.ZipFile(io.BytesIO(resp.content)) as zf:\n",
    "        for file_info in zf.infolist():\n",
    "            filename = file_info.filename\n",
    "            if not filename.lower().endswith((\".md\", \".mdx\")):\n",
    "                continue\n",
    "            try:\n",
    "                with zf.open(file_info) as f_in:\n",
    "                    content = f_in.read().decode(\"utf-8\", errors=\"replace\")\n",
    "                    post = frontmatter.loads(content)\n",
    "                    data = post.to_dict()\n",
    "                    data.update({\n",
    "                        \"filename\": filename,\n",
    "                        \"repo\": repo_name,\n",
    "                        \"owner\": repo_owner,\n",
    "                        \"branch\": branch\n",
    "                    })\n",
    "                    yield data\n",
    "            except Exception as e:\n",
    "                logging.warning(\"Error processing %s: %s\", filename, e)\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a7494ff-4c51-4680-8b4b-0751139ee23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq, size, step):\n",
    "    \"\"\"Yield overlapping chunks from a long string.\"\"\"\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "    n = len(seq)\n",
    "    for i in range(0, n, step):\n",
    "        yield {\"start\": i, \"chunk\": seq[i:i+size]}\n",
    "        if i + size >= n:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a79dc7e-7341-4ca0-b103-0f9916fb5283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading and chunking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 95it [00:01, 61.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Collected 575 chunks. Building index...\n",
      "üéØ Indexing complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üì• Downloading and chunking documents...\")\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in tqdm(read_repo_data(\"evidentlyai\", \"docs\"), desc=\"Processing files\"):\n",
    "    doc_copy = doc.copy()\n",
    "    content = doc_copy.pop(\"content\", \"\")\n",
    "    for chunk in sliding_window(content, size=2000, step=1000):\n",
    "        chunk.update(doc_copy)\n",
    "        evidently_chunks.append(chunk)\n",
    "\n",
    "print(f\"‚úÖ Collected {len(evidently_chunks)} chunks. Building index...\")\n",
    "\n",
    "index = Index(\n",
    "    text_fields=[\"chunk\", \"title\", \"description\", \"filename\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "index.fit(evidently_chunks)\n",
    "\n",
    "print(\"üéØ Indexing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46836ed8-7782-4545-a40d-c86a51222df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîå OpenRouter Connection Test: \n"
     ]
    }
   ],
   "source": [
    "openai_client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\")  # Changed from OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "# Test the connection (optional)\n",
    "try:\n",
    "    test_response = openai_client.chat.completions.create(\n",
    "        model=\"deepseek/deepseek-r1:free\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Say 'Connection successful' if you can read this.\"}],\n",
    "        max_tokens=50\n",
    "    )\n",
    "    print(f\"üîå OpenRouter Connection Test: {test_response.choices[0].message.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection Error: {e}\")\n",
    "    print(\"Make sure you have OPENROUTER_API_KEY in your .env file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "178d1699-ec0c-4073-987c-a47fd1a68bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Found 5 results for query: 'test dataset'\n",
      "Sample result: Retrieval-Augmented Generation (RAG) systems rely on retrieving answers from a knowledge base before generating responses. To evaluate them effectively, you need a test dataset that reflects what the ...\n"
     ]
    }
   ],
   "source": [
    "def text_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Perform a text-based search on the FAQ index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: A list of up to 5 search results returned by the FAQ index.\n",
    "    \"\"\"\n",
    "    results = index.search(query, num_results=5)\n",
    "    print(f\"üîç Found {len(results)} results for query: '{query}'\")\n",
    "    return results\n",
    "\n",
    "# Test the search function\n",
    "test_results = text_search(\"test dataset\")\n",
    "if test_results:\n",
    "    print(f\"Sample result: {test_results[0].get('chunk', '')[:200]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a45f43ee-427f-4632-8aa5-8d7ee7f7fc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question_manual(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Answer a question by manually searching and then using the LLM.\n",
    "    This approach doesn't require tool support from the model.\n",
    "    \"\"\"\n",
    "    # First, search for relevant information\n",
    "    search_results = text_search(question)\n",
    "    \n",
    "    # Format the search results as context\n",
    "    context = \"\\n\\n---\\n\\n\".join([\n",
    "        f\"Result {i+1} (from {result.get('filename', 'unknown')}):\\n{result.get('chunk', '')}\"\n",
    "        for i, result in enumerate(search_results)\n",
    "    ])\n",
    "    \n",
    "    # Create the prompt with the context\n",
    "    prompt = f\"\"\"You are an expert assistant that answers questions about the Evidently project \n",
    "(https://github.com/evidentlyai/evidently) using ONLY the information provided in the context below.\n",
    "\n",
    "Context from Evidently documentation:\n",
    "{context}\n",
    "\n",
    "User question: {question}\n",
    "\n",
    "Instructions:\n",
    "- Answer based ONLY on the provided context\n",
    "- Be concise and clear\n",
    "- If the answer is not in the context, say \"I could not find this information in the Evidently documentation\"\n",
    "- Do not invent features or functionality\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"deepseek/deepseek-r1:free\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions about Evidently.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=1000\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error getting answer: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2c87751-6e85-4cec-aee6-49fc0bc9f46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Question: What components are required in a test dataset to evaluate AI?\n",
      "ü§î Thinking (using manual search + LLM approach)...\n",
      "üîç Found 5 results for query: 'What components are required in a test dataset to evaluate AI?'\n",
      "\n",
      "üí° Answer:\n",
      "To evaluate an AI system using Evidently, the required components in a test dataset depend on the use case but generally include:\n",
      "\n",
      "1. **User-like questions** (generated from your knowledge base or scenario descriptions).  \n",
      "   - For RAG systems: questions paired with **ground truth answers** extracted from the knowledge source.  \n",
      "   - Optionally, include the **context** used to generate answers (e.g., source documents).  \n",
      "\n",
      "2. **Adversarial or edge-case inputs** (e.g., tricky queries to test robustness).  \n",
      "\n",
      "3. **Persona-specific inputs** (e.g., questions tailored to specific user types).  \n",
      "\n",
      "These components can be generated directly in Evidently Cloud by:  \n",
      "- Uploading your knowledge base to create RAG test cases (questions + answers).  \n",
      "- Describing scenarios or providing examples to synthesize diverse inputs.  \n",
      "- Refining the dataset by adding variations, removing irrelevant cases, or editing manually.  \n",
      "\n",
      "The final dataset can be saved as a CSV or accessed via the Python API for evaluation.\n"
     ]
    }
   ],
   "source": [
    "question = \"What components are required in a test dataset to evaluate AI?\"\n",
    "\n",
    "print(f\"‚ùì Question: {question}\")\n",
    "print(\"ü§î Thinking (using manual search + LLM approach)...\")\n",
    "\n",
    "answer = answer_question_manual(question)\n",
    "print(f\"\\nüí° Answer:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6107d19-bc52-41d6-95aa-8a82f073622a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50838ff-e46c-4b8a-ad4e-d0b226c31fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9a816e-c018-4d5b-95dc-a40596c03b54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv)",
   "language": "python",
   "name": "uv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
